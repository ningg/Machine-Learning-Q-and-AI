<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 05
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch05/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 05" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 05</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 05" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch05/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch05/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 05" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch05/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 05" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-5-reducing-overfitting-with-data">
        
        
          Chapter 5: Reducing Overfitting with Data <a href="#chapter-5-reducing-overfitting-with-data"></a>
</h1>
<p><span id="ch05" label="ch05"></span></p>
<p><strong>Suppose we train a neural network classifier in a supervised fashion and notice that it suffers from overfitting. What are some of the common ways to reduce overfitting in neural networks through the use of altered or additional data?</strong></p>
<p><em>Overfitting</em>, a common problem in machine learning, occurs when a model
fits the training data too closely, learning its noise and outliers
rather than the underlying pattern. As a result, the model performs well
on the training data but poorly on unseen or test data. While it is
ideal to prevent overfitting, itâs often not possible to completely
eliminate it. Instead, we aim to reduce or minimize overfitting as much
as possible.</p>
<p>The most successful techniques for reducing overfitting revolve around
collecting more high-quality labeled data. However, if collecting more
labeled data is not feasible, we can augment the existing data or
leverage
unlabeled data for pretraining.</p>
<h2 id="common-methods">
        
        
          Common Methods <a href="#common-methods"></a>
</h2>
<p>This chapter summarizes the most prominent examples of dataset-related
techniques that have stood the test of time, grouping them into the
following categories: collecting more data, data augmentation, and
pretraining.</p>
<h3 id="collecting-more-data">
        
        
          Collecting More Data <a href="#collecting-more-data"></a>
</h3>
<p>One of the best ways to reduce overfitting is to collect more
(good-quality) data. We can plot learning curves to find out whether a
given model would benefit from more data. To construct a learning curve,
we train the model to different training set sizes (10 percent, 20
percent, and so on) and evaluate the trained model on the same
fixed-size validation or test set. As shown in
FigureÂ <a data-reference="fig:ch05-fig01" data-reference-type="ref" href="#fig:ch05-fig01">1.1</a>, the validation accuracy
increases as the training set sizes increase. This indicates that we can
improve the modelâs performance by collecting more data.</p>
<figure id="fig:ch05-fig01">
<img src="../images/ch05-fig01.png">
<figcaption>The learning curve plot of a model fit to different
training<br>
set sizes</br></figcaption>
</img></figure>
<p>The gap between training and validation performance indicates the degree
of overfittingâthe more extensive the gap, the more overfitting occurs.
Conversely, the slope indicating an improvement in the validation
performance suggests the model is underfitting and can benefit from more
data. Typically, additional data can decrease both underfitting and
overfitting.</p>
<h3 id="data-augmentation">
        
        
          Data Augmentation <a href="#data-augmentation"></a>
</h3>
<p>Data augmentation refers to generating new data records or features
based on existing data. It allows for the expansion of a dataset without
additional data collection.</p>
<p>Data augmentation allows us to create different versions of the original
input data, which can improve the modelâs generalization performance.
Why? Augmented data can help the model improve its ability to
generalize, since it makes it harder to memorize spurious information
via training examples or featuresâor, in the case of image data, exact
pixel values for specific pixel locations.
FigureÂ <a data-reference="fig:ch05-fig02" data-reference-type="ref" href="#fig:ch05-fig02">1.2</a> highlights common image data
augmentation techniques, including increasing brightness, flipping, and
cropping.</p>
<figure id="fig:ch05-fig02">
<img src="../images/ch05-fig02.png">
<figcaption>A selection of different image data augmentation
techniques</figcaption>
</img></figure>
<p>Data augmentation is usually standard for image data (see
FigureÂ <a data-reference="fig:ch05-fig02" data-reference-type="ref" href="#fig:ch05-fig02">1.2</a>) and text data (discussed
further in ChapterÂ <a data-reference="ch15" data-reference-type="ref" href="../ch15">[ch15]</a>), but data augmentation
methods for tabular data also exist.</p>
<p>Instead of collecting more data or augmenting existing data, it is also
possible to generate new, synthetic data. While more common for image
data and text, generating synthetic data is also possible for tabular
datasets.</p>
<h3 id="pretraining">
        
        
          Pretraining <a href="#pretraining"></a>
</h3>
<p>As discussed in ChapterÂ <a data-reference="ch02" data-reference-type="ref" href="../ch02">[ch02]</a>, self-supervised learning lets us
leverage large, unlabeled datasets to pretrain neural networks. This can
also help reduce overfitting on the smaller target datasets.</p>
<p>As an alternative to self-supervised learning, traditional transfer
learning on large labeled datasets is also an option. Transfer learning
is most
effective if the labeled dataset is closely related to the target
domain. For instance, if we train a model to classify bird species, we
can pretrain a network on a large, general animal classification
dataset. However, if such a large animal classification dataset is
unavailable, we can also pretrain the model on the relatively broad
ImageNet dataset.</p>
<p>A dataset may be extremely small and unsuitable for supervised
learningâfor example, if it contains only a handful of labeled examples
per class. IfÂ our
classifier needs to operate in a context where the collection of
additional labeled data is not feasible, we may also consider few-shot
learning.</p>
<h2 id="other-methods">
        
        
          Other Methods <a href="#other-methods"></a>
</h2>
<p>The previous sections covered the main approaches to using and modifying
datasets to reduce overfitting. However, this is not an exhaustive list.
Other common techniques include:</p>
<ul>
<li>
<p>Feature engineering and normalization</p>
</li>
<li>
<p>The inclusion of adversarial examples and label or feature noise</p>
</li>
<li>
<p>Label smoothing</p>
</li>
<li>
<p>Smaller batch sizes</p>
</li>
<li>
<p>Data augmentation techniques such as Mixup, Cutout, and CutMix</p>
</li>
</ul>
<p>The next chapter covers additional techniques to reduce overfitting from
a model perspective, and it concludes by discussing which regularization
techniques we should consider in practice.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>5-1. Suppose we train an XGBoost model to
classify images based on manually extracted features obtained from
collaborators. The dataset of
labeled training examples is relatively small, but fortunately, our
collaborators also have a labeled training set from an older project on
a related domain. Weâre considering implementing a transfer learning
approach to train the XGBoost model. Is this a feasible option? If so,
how could we do it? (Assume we are allowed to use only XGBoost and not
another classification algorithm or model.)</p>
<p>5-2. Suppose weâre working on the image
classification problem of implementing MNIST-based handwritten digit
recognition. Weâve added a decent amount of data augmentation to try to
reduce overfitting. Unfortunately, we find that the classification
accuracy is much worse than it was before the augmentation. What are
some potential reasons for this?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>Apaperondataaugmentationfortabulardata:DerekSnow,
âDeltaPy: A Framework for Tabular Data Augmentation in Py-
Â thonâ (2020), <a href="https://github.com/firmai/deltapy">https://github.com/firmai/deltapy</a>.</p>
</li>
<li>
<p>The paper proposing the GReaT method for generating synthetic tabular
data using an auto-regressive generative large language
model: Vadim Borisov et al., âLanguage Models Are Realistic Tabular
Data Generatorsâ (2022), <a href="https://arxiv.org/abs/2210.06280">https://arxiv.org/abs/2210.06280</a>.</p>
</li>
<li>
<p>ThepaperproposingtheTabDDPMmethodforgeneratingsynthetictabulardatausingadiffusionmodel:AkimKotelnikovetal.,âTabDDPM:
Modelling Tabular Data with Diffusion Modelsâ (2022),
<a href="https://arxiv.org/abs/2209.15421">https://arxiv.org/abs/2209.15421</a>.</p>
</li>
<li>
<p>Scikit-learnâs user guide offers a section on preprocessing data,
featuring techniques like feature scaling and normalization that can
enhance your modelâs performance:
<a href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a>.</p>
</li>
<li>
<p>A survey on methods for robustly training deep models with noisy
labels that explores techniques to mitigate the impact of incorrect or
misleading target values: Bo Han et al., âA Survey of Label-noise
Representation Learning: Past, Present and Futureâ (2020),
<a href="https://arxiv.org/abs/2011.04406">https://arxiv.org/abs/2011.04406</a>.</p>
</li>
<li>
<p>Theoretical and empirical evidence to support the idea that control-
Â ling the ratio of batch size to learning rate in stochastic gradient
descent is crucial for good modeling performance in deep neural
networks: Fengxiang He, Tongliang Liu, and Dacheng Tao, âControl Batch
Size and Learning Rate to Generalize Well: Theoretical and Empirical
Evidenceâ (2019),
<a href="https://dl.acm.org/doi/abs/10.5555/3454287.3454390">https://dl.acm.org/doi/abs/10.5555/3454287.3454390</a>.</p>
</li>
<li>
<p>Inclusion of adversarial examples, which are input samples designed to
mislead the model, can improve prediction performance by making the
model more robust: Cihang Xie et al., âAdversarial Examples Improve
Image Recognitionâ (2019), <a href="https://arxiv.org/abs/1911.09665">https://arxiv.org/abs/1911.09665</a>.</p>
</li>
<li>
<p>Label smoothing is a regularization technique that mitigates the im-
Â pact of potentially incorrect labels in the dataset by replacing hard
0 and 1 classification targets with softened values: Rafael MÃ¼ller,
Simon Kornblith, and Geoffrey Hinton, âWhen Does Label Smoothing
Help?â (2019), <a href="https://arxiv.org/abs/1906.02629">https://arxiv.org/abs/1906.02629</a>.</p>
</li>
<li>
<p>Mixup, a popular method that trains neural networks on blended data
pairs to improve generalization and robustness: Hongyi Zhang et al.,
âMixup: Beyond Empirical Risk Minimizationâ (2018),
<a href="https://arxiv.org/abs/1710.09412">https://arxiv.org/abs/1710.09412</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
