<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 30
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch30/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 30" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 30</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 30" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch30/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch30/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 30" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch30/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 30" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-30-limited-labeled-data">
        
        
          Chapter 30: Limited Labeled Data <a href="#chapter-30-limited-labeled-data"></a>
</h1>
<p><span id="ch30" label="ch30"></span></p>
<p><strong>Suppose we plot a learning curve (as shown in
FigureÂ <a data-reference="fig:ch05-fig01" data-reference-type="ref" href="#fig:ch05-fig01">[fig:ch05-fig01]</a> on pageÂ , for
example) and find the machine learning model overfits and could benefit
from more training data. What are some different approaches for dealing
with limited labeled data in supervised machine learning settings?</strong></p>
<p>In lieu of collecting more data, there are several methods related to
regular supervised learning that we can use to improve model performance
in limited labeled data regimes.</p>
<h2 id="improving-model-performance-with-limited-labeled-data">
        
        
          Improving Model Performance with Limited Labeled Data <a href="#improving-model-performance-with-limited-labeled-data"></a>
</h2>
<p>The following sections explore various machine learning paradigms that
help in scenarios where training data is limited.</p>
<h3 id="labeling-more-data">
        
        
          Labeling More Data <a href="#labeling-more-data"></a>
</h3>
<p>Collecting additional training examples is often the best way to improve
the performance of a model (a learning curve is a good diagnostic for
this). However, this is often not feasible in practice, because
acquiring high-quality data can be costly, computational resources and
storage might be insufficient, or the data may be hard to access.</p>
<h3 id="bootstrapping-the-data">
        
        
          Bootstrapping the Data <a href="#bootstrapping-the-data"></a>
</h3>
<p>Similar to the techniques for reducing overfitting discussed in
ChapterÂ <a data-reference="ch05" data-reference-type="ref" href="../ch05">[ch05]</a>, it can be helpful to âbootstrapâ the
data by generating modified (augmented) or artificial (synthetic)
training examples to boost the performance of the predictive model. Of
course, improving the quality of data can also lead to the improved
predictive performance of a model, as discussed in
ChapterÂ <a data-reference="ch21" data-reference-type="ref" href="../ch21">[ch21]</a>.</p>
<h3 id="transfer-learning">
        
        
          Transfer Learning <a href="#transfer-learning"></a>
</h3>
<p>Transferlearningdescribestrainingamodelonageneraldataset(forexample,
ImageNet) and then fine-tuning the pretrained target dataset (for
example, a dataset consisting of different bird species), as outlined
in
FigureÂ <a data-reference="fig:ch30-fig01" data-reference-type="ref" href="#fig:ch30-fig01">1.1</a>.</p>
<figure id="fig:ch30-fig01">
<img src="../images/ch30-fig01.png">
<figcaption>The process of transfer learning</figcaption>
</img></figure>
<p>Transfer learning is usually done in the context of deep learning, where
model weights can be updated. This is in contrast to tree-based methods,
since most decision tree algorithms are nonparametric models that do not
support iterative training or parameter updates.</p>
<h3 id="self-supervised-learning">
        
        
          Self-Supervised Learning <a href="#self-supervised-learning"></a>
</h3>
<p>Similar to transfer learning, in self-supervised learning, the model is
pretrained on a different task before being fine-tuned to a target task
for which only limited data exists. However, self-supervised learning
usually relies on label information that can be directly and
automatically extracted from unlabeled data. Hence, self-supervised
learning is also often called <em>unsupervised pretraining</em>.</p>
<p>Common examples of self-supervised learning include the <em>next word</em>
(used in GPT, for example) or <em>masked word</em> (used in BERT, for example)
pretraining tasks in language modeling, covered in more detail in
ChapterÂ <a data-reference="ch17" data-reference-type="ref" href="../ch17">[ch17]</a>. Another intuitive example from
computer vision includes <em>inpainting</em>: predicting the missing part of an
image that was randomly removed, illustrated in
FigureÂ <a data-reference="fig:ch30-fig02" data-reference-type="ref" href="#fig:ch30-fig02">1.2</a>.</p>
<figure id="fig:ch30-fig02">
<img src="../images/ch30-fig02.png">
<figcaption>Inpainting for self-supervised learning</figcaption>
</img></figure>
<p>For more detail on self-supervised learning, see
ChapterÂ <a data-reference="ch02" data-reference-type="ref" href="../ch02">[ch02]</a>.</p>
<h3 id="active-learning">
        
        
          Active Learning <a href="#active-learning"></a>
</h3>
<p>In active learning, illustrated in
FigureÂ <a data-reference="fig:ch30-fig03" data-reference-type="ref" href="#fig:ch30-fig03">1.3</a>, we typically involve manual
labelers or users for feedback during the learning process. However,
instead of labeling the entire dataset up front, active learning
includes a prioritization scheme for suggesting unlabeled data points
for labeling to maximize the machine learning modelâs performance.</p>
<figure id="fig:ch30-fig03">
<img src="../images/ch30-fig03.png">
<figcaption>In active learning, a model queries an<br>
oracle for labels.</br></figcaption>
</img></figure>
<p>The term <em>active learning</em> refers to the fact that the model actively
selects data for labeling. For example, the simplest form of active
learning selects data points with high prediction uncertainty for
labeling by a human annotator (also referred to as an <em>oracle</em>).</p>
<h3 id="few-shot-learning">
        
        
          Few-Shot Learning <a href="#few-shot-learning"></a>
</h3>
<p>In a few-shot learning scenario, we often deal with extremely small datasets that include only a handful of examples per class. In research contexts, 1-shot(one example per class) and 5-shot (five examples per class) learning scenarios are very common.
An extreme case of few-shot learning is zero-shot learning, where no labels are provided. Popular examples of zero-shot learning include GPT-3 and related language models, where the user has to provide all the necessary information via the input prompt, as illustrated in FigureÂ <a data-reference="fig:ch30-fig04" data-reference-type="ref" href="#fig:ch30-fig04">1.4</a>.</p>
<figure id="fig:ch30-fig04">
<figcaption>Zero-shot classification with ChatGPT</figcaption>
</figure>
<p>For more detail on few-shot learning, see
ChapterÂ <a data-reference="ch03" data-reference-type="ref" href="../ch03">[ch03]</a>.</p>
<h3 id="meta-learning">
        
        
          Meta-Learning <a href="#meta-learning"></a>
</h3>
<p>Meta-learning involves developing methods that determine how machine
learning algorithms can best learn from data. We can therefore think of
meta-learning as âlearning to learn.â The machine learning community has
developed several approaches for meta-learning. Within the machine
learning community, the term <em>meta-learning</em> doesnât just represent
multiple subcategories and approaches; it is also occasionally employed
to describe related yet distinct processes, leading to nuances in its
interpretation and application.</p>
<p>Meta-learning is one of the main subcategories of few-shot learning.
Here, the focus is on learning a good feature extraction module, which
converts support and query images into vector representations. These
vector representations are optimized for determining the predicted class
of the query example via comparisons with the training examples in the
support set. (This form of meta-learning is illustrated in
ChapterÂ <a data-reference="ch03" data-reference-type="ref" href="../ch03">[ch03]</a> on pageÂ .) Another branch of
meta-learning unrelated to the few-shot learning approach is focused on
extracting metadata (also called <em>meta-features</em>) from datasets for
supervised learning tasks, as illustrated in
FigureÂ <a data-reference="fig:ch30-fig05" data-reference-type="ref" href="#fig:ch30-fig05">1.5</a>. The meta-features are
descriptions of the dataset itself. For example, these can include the
number of features and statistics of the different features (kurtosis,
range, mean, and so on).</p>
<figure id="fig:ch30-fig05">
<img src="../images/ch30-fig05.png">
<figcaption>The meta-learning process involving the extraction of
metadata</figcaption>
</img></figure>
<p>The extracted meta-features provide information for selecting a machine
learning algorithm for the dataset at hand. Using this approach, we can
narrow down the algorithm and hyperparameter search spaces, which helps
reduce overfitting when the dataset is small.</p>
<h3 id="weakly-supervised-learning">
        
        
          Weakly Supervised Learning <a href="#weakly-supervised-learning"></a>
</h3>
<p>Weakly supervised learning, illustrated in
FigureÂ <a data-reference="fig:ch30-fig06" data-reference-type="ref" href="#fig:ch30-fig06">1.6</a>, involves using an external
label source to generate labels for an unlabeled dataset. Often, the
labels created by a weakly supervised labeling function are more noisy
or inaccurate than those produced by a human or domain expert, hence the
term <em>weakly</em> supervised. We can develop or adopt a rule-based
classifier to create the labels in weakly supervised learning; these
rules usually cover only a subset of the unlabeled dataset.</p>
<figure id="fig:ch30-fig06">
<img src="../images/ch30-fig06.png">
<figcaption>Weakly supervised learning uses external labeling functions
to<br>
train machine learning models.</br></figcaption>
</img></figure>
<p>LetâsreturntotheexampleofemailspamclassificationfromChapterÂ <a data-reference="ch23" data-reference-type="ref" href="../ch23">[ch23]</a> to illustrate a rule-based approach for
data labeling. In weak supervision, we could design a rule-based
classifier based on the keyword <em>SALE</em> in the email subject header line
to identify a subset of spam emails. Note that while we may use this
rule to label certain emails as spam positive, we should not apply this
rule to label emails without <em>SALE</em> as non-spam. Instead, we should
either leave those unlabeled or apply a different rule to them.</p>
<p>There is a subcategory of weakly supervised learning referred to as
PU-learning. In <em>PU-learning</em>, which is short for <em>positive-unlabeled
learning</em>,
we label and learn only from positive examples.</p>
<h3 id="semi-supervised-learning">
        
        
          Semi-Supervised Learning <a href="#semi-supervised-learning"></a>
</h3>
<p>Semi-supervised learning is closely related to weakly supervised
learning: it also involves creating labels for unlabeled instances in
the dataset. The main difference between these two methods lies in <em>how</em>
we create the labels. In weak supervision, we create labels using an
external labeling function that is often noisy, inaccurate, or covers
only a subset of the data. In semi-supervision, we do not use an
external label function; instead, we leverage the structure of the data
itself. We can, for example, label additional data points based on the
density of neighboring labeled data points, as illustrated in
FigureÂ <a data-reference="fig:ch30-fig07" data-reference-type="ref" href="#fig:ch30-fig07">1.7</a>.</p>
<figure id="fig:ch30-fig07">
<img src="../images/ch30-fig07.png">
<figcaption>Semi-supervised learning</figcaption>
</img></figure>
<p>While we can apply weak supervision to an entirely unlabeled dataset,
semi-supervised learning requires at least a portion of the data to be
labeled. In practice, it is possible first to apply weak supervision to
label a subset of the data and then to use semi-supervised learning to
label instances that were not captured by the labeling functions.</p>
<p>Thanks to their close relationship, semi-supervised learning is sometimes referred
to as a subcategory of weakly supervised learning, and vice
versa.</p>
<h3 id="self-training">
        
        
          Self-Training <a href="#self-training"></a>
</h3>
<p>Self-training falls somewhere between semi-supervised learning and
weakly supervised learning. For this technique, we train a model to
label the dataset or adopt an existing model to do the same. This model
is also referred to as a <em>pseudo-labeler</em>.</p>
<p>Self-training does not guarantee accurate labels and is thus related to
weakly supervised learning. Moreover, while we use or adopt a machine
learning model for this pseudo-labeling, self-training is also related
to semi-supervised learning.</p>
<p>An example of self-training is knowledge distillation, discussed in
ChapterÂ <a data-reference="ch06" data-reference-type="ref" href="../ch06">[ch06]</a>.</p>
<h3 id="multi-task-learning">
        
        
          Multi-Task Learning <a href="#multi-task-learning"></a>
</h3>
<p>Multi-task learning trains neural networks on multiple, ideally related
tasks. For example, if we are training a classifier to detect spam
emails, spam classification is the main task. In multi-task learning, we
can add one or more related tasks for the model to solve, referred to as
<em>auxiliary tasks</em>. For the spam email example, an auxiliary task could
be classifying the emailâs topic or language.</p>
<p>Typically, multi-task learning is implemented via multiple loss
functions that have to be optimized simultaneously, with one loss
function for each task. The auxiliary tasks serve as an inductive bias,
guiding the model to prioritize hypotheses that can explain multiple
tasks. This approach often results in models that perform better on
unseen data. There are two subcategories of multi-task learning:
multi-task learning with hard parameter sharing and multi-task learning
with soft parameter sharing.
FigureÂ <a data-reference="fig:ch30-fig08" data-reference-type="ref" href="#fig:ch30-fig08">[fig:ch30-fig08]</a> illustrates the
difference between these two methods.</p>
<div class="figurewide">
<img alt="image" src="../images/ch30-fig08.png" style="width:5.625in"/>
</div>
<p>In <em>hard</em> parameter sharing, as shown in
FigureÂ <a data-reference="fig:ch30-fig08" data-reference-type="ref" href="#fig:ch30-fig08">[fig:ch30-fig08]</a>, only the output
layers are task specific, while all the tasks share the same hidden
layers and neural network backbone architecture. In contrast, <em>soft</em>
parameter sharing uses separate neural networks for each task, but
regularization techniques such as distance minimization between
parameter layers are applied to encourage similarity among the networks.</p>
<h3 id="multimodal-learning">
        
        
          Multimodal Learning <a href="#multimodal-learning"></a>
</h3>
<p>While multi-task learning involves training a model with multiple tasks
and loss functions, multimodal learning focuses on incorporating
multiple types of input data.</p>
<p>Common examples of multimodal learning are architectures that take both
image and text data as input (though multimodal learning is not
restricted to only two modalities and can be used for any number of
input modalities). Depending on the task, we may employ a matching loss
that forces the embedding vectors between related images and text to be
similar, as shown in
FigureÂ <a data-reference="fig:ch30-fig09" data-reference-type="ref" href="#fig:ch30-fig09">1.8</a>. (See
ChapterÂ <a data-reference="ch01" data-reference-type="ref" href="../ch01">[ch01]</a> for more on embedding vectors.)</p>
<figure id="fig:ch30-fig09">
<img src="../images/ch30-fig09.png"/>
<figcaption>Multimodal learning with a matching loss</figcaption>
</figure>
<p>FigureÂ <a data-reference="fig:ch30-fig09" data-reference-type="ref" href="#fig:ch30-fig09">1.8</a> shows image and text encoders as
separate components. The image encoder can be a convolutional backbone
or a vision transformer, and the language encoder can be a recurrent
neural network or language transformer. However, itâs common nowadays to
use a single transformer-based module that can simultaneously process
image and text data. For example, the VideoBERT model has a joint module
that processes both video and text for action classification and video
captioning.</p>
<p>Optimizing a matching loss, as shown in
FigureÂ <a data-reference="fig:ch30-fig09" data-reference-type="ref" href="#fig:ch30-fig09">1.8</a>, can be useful for learning
embeddings that can be applied to various tasks, such as image
classification or summarization. However, it is also possible to
directly optimize the target loss, like classification or regression, as
FigureÂ <a data-reference="fig:ch30-fig10" data-reference-type="ref" href="#fig:ch30-fig10">1.9</a> illustrates.</p>
<figure id="fig:ch30-fig10">
<img src="../images/ch30-fig10.png"/>
<figcaption>Multimodal learning for optimizing a supervised<br>
learning objective</br></figcaption>
</figure>
<p>FigureÂ <a data-reference="fig:ch30-fig10" data-reference-type="ref" href="#fig:ch30-fig10">1.9</a> shows data being collected from
two different sensors. One could be a thermometer and the other could be
a video camera. The signal encoders convert the information into
embeddings (sharing the same number of dimensions), which are then
concatenated to form the input representation for the model.</p>
<p>Intuitively, models that combine data from different modalities generally
perform better than unimodal models because they can leverage more information.
Moreover, recent research suggests that the key to the sucess of multimodal
learning is the improved quality of the latent space
representation.</p>
<h3 id="inductive-biases">
        
        
          Inductive Biases <a href="#inductive-biases"></a>
</h3>
<p>Choosing models with stronger inductive biases can help lower data
requirements by making assumptions about the structure of the data. For
example, due to their inductive biases, convolutional networks require
less data than vision transformers, as discussed in
ChapterÂ <a data-reference="ch13" data-reference-type="ref" href="../ch13">[ch13]</a>.</p>
<h2 id="recommendations">
        
        
          Recommendations <a href="#recommendations"></a>
</h2>
<p>Of all these techniques for reducing data requirements, how should we
decide which ones to use in a given situation?</p>
<p>Techniques like collecting more data, data augmentation, and feature
engineering are compatible with all the methods discussed in this
chapter. Multi-task learning and multimodal inputs can also be used with
the learning strategies outlined here. If the model suffers from
overfitting, we should also include techniques discussed in
ChaptersÂ <a data-reference="ch05" data-reference-type="ref" href="../ch05">[ch05]</a> and
<a data-reference="ch06" data-reference-type="ref" href="../ch06">[ch06]</a>.</p>
<p>But how can we choose between active learning, few-shot learning,
transfer learning, self-supervised learning, semi-supervised learning,
and weakly supervised learning? Deciding which supervised learning
technique(s) to try is highly context dependent. You can use the diagram
in FigureÂ <a data-reference="fig:ch30-fig11" data-reference-type="ref" href="#fig:ch30-fig11">1.10</a> as a guide to choosing the best
method for your particular project.</p>
<figure id="fig:ch30-fig11">
<img src="../images/ch30-fig11.png"/>
<figcaption>Recommendations for choosing a supervised learning<br>
technique</br></figcaption>
</figure>
<p>Note that the dark boxes in
FigureÂ <a data-reference="fig:ch30-fig11" data-reference-type="ref" href="#fig:ch30-fig11">1.10</a> are not terminal nodes but arc
back to the second box, âEvaluate model performanceâ; additional arrows
were omitted to avoid visual clutter.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>30-1. Suppose we are given the task of
constructing a machine learning model that utilizes images to detect
manufacturing defects on the outer shells of tablet devices similar to
iPads. We have access to millions of images of various computing
devices, including smartphones, tablets, and computers, which are not
labeled; thousands of labeled pictures of smartphones depicting various
types of damage; and hundreds of labeled
images specifically related to the target task of detecting
manufacturing defects on tablet devices. How could we approach this
problem using self-supervised learning or transfer learning?</p>
<p>30-2. In active learning, selecting difficult
examples for human inspection and labeling is often based on confidence
scores. Neural networks can provide such scores by using the logistic
sigmoid or softmax function in the output layer to calculate
class-membership probabilities. However, it is widely recognized that
deep neural networks exhibit overconfidence on out-of-distribution data,
rendering their use in active learning ineffective. What are some other
methods to obtain confidence scores using deep neural networks for
active learning?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>While decision trees for incremental learning are not commonly
implemented, algorithms for training decision trees in an itera-
Â tive fashion do exist:
<a href="https://en.wikipedia.org/wiki/Incremental_decision_tree"><em>https://en.wikipedia.org/wiki/Incremental</em></a>
<a href="https://en.wikipedia.org/wiki/Incremental_decision_tree"><em>_decision_tree</em></a>.</p>
</li>
<li>
<p>Models trained with multi-task learning often outperform models
trained on a single task: Rich Caruana, âMultitask Learningâ (1997),
<a href="https://doi.org/10.1023%2FA%3A1007379606734">https://doi.org/10.1023%2FA%3A1007379606734</a>.</p>
</li>
<li>
<p>A single transformer-based module that can simultaneously process
image and text data: Chen Sun et al., âVideoBERT: A Joint Model for
Video and Language Representation Learningâ (2019),
<a href="https://arxiv.org/abs/1904.01766">https://arxiv.org/abs/1904.01766</a>.</p>
</li>
<li>
<p>The aforementioned research suggesting the key to the success of
multimodal learning is the improved quality of the latent space
representation: Yu Huang et al., âWhat Makes Multi-Modal Learning
Better Than Single (Provably)â (2021),
<a href="https://arxiv.org/abs/2106.04538">https://arxiv.org/abs/2106.04538</a>.</p>
</li>
<li>
<p>For more information on active learning: Zhen et al., âA Comparative
Survey of Deep Active Learningâ (2022),
<a href="https://arxiv.org/abs/2203.13450">https://arxiv.org/abs/2203.13450</a>.</p>
</li>
<li>
<p>For a more detailed discussion on how out-of-distribution data can
lead to overconfidence in deep neural networks: Anh Nguyen, Jason
Yosinski, and Jeff Clune, âDeep Neural Networks Are Easily Fooled:
High Confidence Predictions for Unrecognizable Imagesâ (2014),
<a href="https://arxiv.org/abs/1412.1897">https://arxiv.org/abs/1412.1897</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
