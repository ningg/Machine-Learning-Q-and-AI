<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 06
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch06/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 06" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 06</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 06" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch06/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch06/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 06" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch06/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 06" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-6-reducing-overfitting-with-model-modifications">
        
        
          Chapter 6: Reducing Overfitting with Model Modifications <a href="#chapter-6-reducing-overfitting-with-model-modifications"></a>
</h1>
<p><span id="ch06" label="ch06"></span></p>
<p><strong>Suppose we train a neural network classifier in a supervised fashion and
already employ various dataset-related techniques to mitigate
overfitting. How can we change the model or make modifications to the
training loop to further reduce the effect of overfitting?</strong></p>
<p>The most successful approaches against overfitting include
regularization techniques like dropout and weight decay. As a rule of
thumb, models with a larger number of parameters require more training
data to generalize well. Hence, decreasing the model size and capacity
can sometimes also help reduce overfitting. Lastly, building ensemble
models is among the most effective ways to combat overfitting, but it
comes with increased computational expense.</p>
<p>This chapter outlines the key ideas and techniques for several
categories of reducing overfitting with model modifications and then
compares them to one another. It concludes by discussing how to choose
between all types of overfitting reduction methods, including those
discussed in the previous chapter.</p>
<h2 id="common-methods">
        
        
          Common Methods <a href="#common-methods"></a>
</h2>
<p>The various model- and training-related techniques to reduce
overfitting
can be grouped into three broad categories: (1) adding regularization,
(2) choosing smaller models, and (3) building ensemble models.</p>
<h3 id="regularization">
        
        
          Regularization <a href="#regularization"></a>
</h3>
<p>We can interpret regularization as a penalty against complexity. Classic
regularization techniques for neural networks include <em>L</em><sub>2</sub>
regularization and the related weight decay method. We implement
<em>L</em><sub>2</sub> regularization by adding a penalty term to the loss
function that is minimized during training. This added term represents
the size of the weights, such as the squared sum of the weights. The
following formula shows an <em>L</em><sub>2</sub> regularized loss</p>

\[RegularizedLoss=Loss+\frac{\lambda}{n} \sum_j w_{j}^{2}\]

<p>where \(\lambda\) is a hyperparameter that controls the regularization
strength.</p>
<p>During backpropagation, the optimizer minimizes the modified loss, now
including the additional penalty term, which leads to smaller model
weights and can improve generalization to unseen data.</p>
<p>Weight decay is similar to <em>L</em><sub>2</sub> regularization but is applied
to the optimizer directly rather than modifying the loss function. Since
weight decay has the same effect as <em>L</em><sub>2</sub> regularization, the
two methods are often used
synonymously, but there may be subtle differences depending on the
implementation details and optimizer.</p>
<p>Many other techniques have regularizing effects. For brevityâs sake,
weâll discuss just two more widely used methods: dropout and early
stopping.</p>
<p>Dropout reduces overfitting by randomly setting some of the activations
of the hidden units to zero during training. Consequently, the neural
network cannot rely on particular neurons to be activated. Instead, it
learns to use a larger number of neurons and multiple independent
representations of the same data, which helps to reduce overfitting.</p>
<p>In early stopping, we monitor the modelâs performance on a validation
set during training and stop the training process when the performance
on the validation set begins to decline, as illustrated in
FigureÂ <a data-reference="fig:ch06-fig01" data-reference-type="ref" href="#fig:ch06-fig01">1.1</a>.</p>
<figure id="fig:ch06-fig01">
<img src="../images/ch06-fig01.png">
<figcaption>Early stopping</figcaption>
</img></figure>
<p>In FigureÂ <a data-reference="fig:ch06-fig01" data-reference-type="ref" href="#fig:ch06-fig01">1.1</a>, we can see that the validation
accuracy increases as the training and validation accuracy gap closes.
The point where the training and validation accuracy is closest is the
point with the least amount of overfitting, which is usually a good
point for early stopping.</p>
<h3 id="smaller-models">
        
        
          Smaller Models <a href="#smaller-models"></a>
</h3>
<p>Classic bias-variance theory suggests that reducing model size can
reduce overfitting. The intuition behind this theory is that, as a
general rule of thumb, the smaller the number of model parameters, the
smaller its capacity to memorize or overfit to noise in the data. The
following paragraphs discuss methods to reduce the model size, including
pruning, which removes parameters from a model, and knowledge
distillation, which transfers knowledge to a smaller model.</p>
<p>Besides reducing the number of layers and shrinking the layersâ widths
as a hyperparameter tuning procedure, another approach to obtaining
smal-
Â ler models is <em>iterative pruning</em>, in which we train a large model to
achieve good performance on the original dataset. We then iteratively
remove parameters of the model, retraining it on the dataset such that
it maintains the same predictive performance as the original model. (The
lottery ticket hypothesis, discussed in
ChapterÂ <a data-reference="ch04" data-reference-type="ref" href="../ch04">[ch04]</a>, uses iterative pruning.)</p>
<p>Another common approach to obtaining smaller models is <em>knowledge
distillation</em>. The general idea behind this approach is to transfer
knowledge from a large, more complex model (the <em>teacher</em>) to a smaller
model (the
<em>student</em>). Ideally, the student achieves the same predictive accuracy
as the teacher, but it does so more efficiently due to the smaller size.
As a nice side effect, the smaller student may overfit less than the
larger teacher model.
FigureÂ <a data-reference="fig:ch06-fig02" data-reference-type="ref" href="#fig:ch06-fig02">[fig:ch06-fig02]</a> diagrams the
original knowledge distillation process. Here, the teacher is first
trained in a regular supervised fashion to classify the examples in the
dataset well, using a conventional cross-entropy loss between the
predicted scores and ground truth class labels. While the smaller
student network is trained on the same dataset, the training objective
is to minimize both (a) the cross entropy between the outputs and the
class labels and (b) the difference between its outputs and the teacher
outputs (measured using <em>KullbackâLeibler</em> divergence, which quantifies
the difference between two probability distributions by calculating how
much one distribution diverges from the other in terms of information
content).</p>
<div class="figurewide">
<img alt="image" src="../images/ch06-fig02.png" style="width:5.625in">
</img></div>
<p>By minimizing the KullbackâLeibler divergenceâthe difference between the
teacher and student score distributionsâthe student learns to mimic the
teacher while being smaller and more efficient.</p>
<h3 id="caveats-with-smaller-models">
        
        
          Caveats with Smaller Models <a href="#caveats-with-smaller-models"></a>
</h3>
<p>While pruning and knowledge distillation can also enhance a modelâs
generalization performance, these techniques are not primary or
effective ways of reducing overfitting.</p>
<p>Early research results indicate that pruning and knowledge distillation
can improve the generalization performance, presumably due to smaller
model sizes. However, counterintuitively, recent research studying
phenomena like double descent and grokking also showed that larger,
overparameterized models have improved generalization performance if
they are trained beyond the point of overfitting. <em>Double descent</em>
refers to the phenomenon in which models with either a small or an
extremely large number of para-
Â meters have good generalization performance, while models with a number
of parameters equal to the number of training data points have poor
generalization performance. <em>Grokking</em> reveals that as the size of a
dataset decreases, the need for optimization increases, and
generalization performance can improve well past the point of
overfitting.</p>
<p>How can we reconcile the observation that pruned models can exhibit
better generalization performance with contradictory observations from
studies of double descent and grokking? Researchers recently showed that
the improved training process partly explains the reduction of
overfitting due to pruning. Pruning involves more extended training
periods and a replay of learning rate schedules that may be partly
responsible for the improved generalization performance.</p>
<p>Pruning and knowledge distillation remain excellent ways to improve the
computational efficiency of a model. However, while they can also
enhance a modelâs generalization performance, these techniques are not
primary or effective ways of reducing overfitting.</p>
<h3 id="ensemble-methods">
        
        
          Ensemble Methods <a href="#ensemble-methods"></a>
</h3>
<p>Ensemble methods combine predictions from multiple models to improve the
overall prediction performance. However, the downside of using multiple
models is an increased computational cost.</p>
<p>We can think of ensemble methods as asking a committee of experts to
weigh in on a decision and then combining their judgments in some way to
make a final decision. Members in a committee often have different
backgrounds and experiences. While they tend to agree on basic
decisions, they can overrule bad decisions by majority rule. This
doesnât mean that the majority of experts is always right, but there is
a good chance that the majority of the committee is more often right, on
average, than every single member.</p>
<p>The most basic example of an ensemble method is majority voting. Here,
we train <em>k</em> different classifiers and collect the predicted class label
from each of these <em>k</em> models for a given input. We then return the most
frequent class label as the final prediction. (Ties are usually resolved
using a confidence score, randomly picking a label, or picking the class
label with the lowest index.)</p>
<p>Ensemble methods are more prevalent in classical machine learning than
deep learning because it is more computationally expensive to employ
multiple models than to rely on a single one. In other words, deep
neural networks require significant computational resources, making them
less suitable for ensemble methods.</p>
<p>Random forests and gradient boosting are popular examples of ensemble methods.
However, by using majority voting or stacking, for example, we can combine any group of models:
an ensemble may consist of a support vector machine, a multilayer perceptron, and a nearest-neighbor classifier.
Here, stacking (also known as <em>stacked generalization</em>)is a more advanced variant of majority voting that involves training a new model to combine the predictions of several other models rather than obtaining the label by majorit yvote.</p>
<p>A popular industry technique is to build models from <em>k-fold cross-validation</em>, a model evaluationt
echnique in which we train and evaluate a model on <em>k</em> training folds.We then compute the average performance metric across all <em>k</em> iterations to estimate the overall performance measure of the model. After evaluation, we can either train the model on the entire training dataset or combine the individual models as an ensemble, as shown in FigureÂ <a data-reference="fig:ch06-fig03" data-reference-type="ref" href="#fig:ch06-fig03">1.2</a>.</p>
<figure id="fig:ch06-fig03">
<img src="../images/ch06-fig03.png">
<figcaption><span class="upright">k</span>-fold cross-validation for
creating model ensembles</figcaption>
</img></figure>
<p>As shown in FigureÂ <a data-reference="fig:ch06-fig03" data-reference-type="ref" href="#fig:ch06-fig03">1.2</a>, the <em>k</em>-fold ensemble approach
trains each of the <em>k</em> models on the respective <em>k</em> â 1 training folds
in each round. After evaluating the models on the validation folds, we
can combine them into a majority vote classifier or build an ensemble
using stacking, a technique that combines multiple classification or
regression models via a meta-model.</p>
<p>While the ensemble approach can potentially reduce overfitting and
improve robustness, this approach is not always suitable. For instance,
potential downsides include managing and deploying an ensemble of
models, which can be more complex and computationally expensive than
using a
single model.</p>
<h2 id="other-methods">
        
        
          Other Methods <a href="#other-methods"></a>
</h2>
<p>So far, this book has covered some of the most prominent techniques to
reduce overfitting. ChapterÂ <a data-reference="ch05" data-reference-type="ref" href="../ch05">[ch05]</a> covered techniques that aim to reduce
overfitting from a data perspective. Additional techniques for reducing
overfitting with model modifications include skip-connections (found in
residual networks, for example), look-ahead optimizers, stochastic
weight averaging, multitask learning, and snapshot ensembles.</p>
<p>While they are not originally designed to reduce overfitting, layer
input normalization techniques such as batch normalization (BatchNorm)
and layer normalization (LayerNorm) can stabilize training and often
have a regularizing effect that reduces overfitting. Weight
normalization, which normalizes the model weights instead of layer
inputs, could also lead to better generalization performance. However,
this effect is less direct since weight normalization (WeightNorm)
doesnât explicitly act as a regularizer like weight decay does.</p>
<h2 id="choosing-a-regularization-technique">
        
        
          Choosing a Regularization Technique <a href="#choosing-a-regularization-technique"></a>
</h2>
<p>Improving data quality is an essential first step in reducing
overfitting. However, for recent deep neural networks with large numbers
of parameters, we need to do more to achieve an acceptable level of
overfitting. Therefore, data augmentation and pretraining, along with
established techniques such as dropout and weight decay, remain crucial
overfitting reduction methods.</p>
<p>In practice, we can and should use multiple methods at once to reduce
overfitting for an additive effect. To achieve the best results, treat
selecting these techniques as a hyperparameter optimization problem.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>6-1.
Supposeweâreusingearlystoppingasamechanismtoreduceover-
Â fittingâinparticular,amodernearly-stoppingvariantthatcreates
checkpoints of the best model (for instance, the model with the high-
Â est validation accuracy) during training so that we can load it after
the training has completed. This mechanism can be enabled in most modern
deep learning frameworks. However, a colleague recommends
tuning the number of training epochs instead. What are some of the
advantages and disadvantages of each approach?</p>
<p>6-2. Ensemble models have been established as a
reliable and successful method for decreasing overfitting and enhancing
the reliability of predictive modeling efforts. However, thereâs always
a trade-off. What are some of the drawbacks associated with ensemble
techniques?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>For more on the distinction between <em>L</em><sub>2</sub> regularization and
weight decay: Guodong Zhang et al., âThree Mechanisms of Weight Decay
Regularizationâ (2018), <a href="https://arxiv.org/abs/1810.12281">https://arxiv.org/abs/1810.12281</a>.</p>
</li>
<li>
<p>Research results indicate that pruning and knowledge distillation can
improve generalization performance, presumably due to smaller model
sizes: Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, âDistilling the
Knowledge in a Neural Networkâ (2015),
<a href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a>.</p>
</li>
<li>
<p>Classic bias-variance theory suggests that reducing model size can
reduce overfitting: Jerome H. Friedman, Robert Tibshirani, and Trevor
Hastie, âModel Selection and Bias-Variance Tradeoff,â
Chapter 2.9, in <em>The Elements of Statistical Learning</em> (Springer,
2009).</p>
</li>
<li>
<p>The lottery ticket hypothesis applies knowledge distillation to find
smaller networks with the same predictive performance as the original
one: Jonathan Frankle and Michael Carbin, âThe Lottery Ticket
Hypothesis: Finding Sparse, Trainable Neural Networksâ (2018),
<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>.</p>
</li>
<li>
<p>For more on double descent:
<a href="https://en.wikipedia.org/wiki/Double_descent">https://en.wikipedia.org/wiki/Double_descent</a>.</p>
</li>
<li>
<p>The phenomenon of grokking indicates that generalization perfor-
Â mance can improve well past the point of overfitting: Alethea Power
et al., âGrokking: Generalization Beyond Overfitting on Small
Algorithmic Datasetsâ (2022), <a href="https://arxiv.org/abs/2201.02177">https://arxiv.org/abs/2201.02177</a>.</p>
</li>
<li>
<p>Recent research shows that the improved training process partly
explains the reduction of overfitting due to pruning: Tian Jin et al.,
âPruningâs Effect on Generalization Through the Lens of Training and
Regularizationâ (2022), <a href="https://arxiv.org/abs/2210.13738">https://arxiv.org/abs/2210.13738</a>.</p>
</li>
<li>
<p>Dropout was previously discussed as a regularization technique,
but it can also be considered an ensemble method that approximates a
weighted geometric mean of multiple networks: Pierre Baldi and
Peter J. Sadowski, âUnderstanding Dropoutâ (2013),
<a href="https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html"><em>https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599</em></a>
<a href="https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html"><em>e06ad9bf1ba03cb0-Abstract.html</em></a>.</p>
</li>
<li>
<p>Regularization cocktails need to be tuned on a per-dataset basis:
Arlind Kadra et al., âWell-Tuned Simple Nets Excel on Tabular
Datasetsâ (2021), <a href="https://arxiv.org/abs/2106.11189">https://arxiv.org/abs/2106.11189</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
