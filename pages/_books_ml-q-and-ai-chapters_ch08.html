<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 08
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch08/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 08" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 08</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 08" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch08/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch08/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 08" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch08/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 08" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-8-the-success-of-transformers">
        
        
          Chapter 8: The Success of Transformers <a href="#chapter-8-the-success-of-transformers"></a>
</h1>
<p><span id="ch08" label="ch08"></span></p>
<p><strong>What are the main factors that have contributed to the success of
transformers?</strong></p>
<p>In recent years, transformers have emerged as the most successful
neural network architecture, particularly for various natural language
processing tasks. In fact, transformers are now on the cusp of becoming
state of the art for computer vision tasks as well. The success of
transformers can be attributed to several key factors, including their
attention mechanisms, ability to be parallelized easily, unsupervised
pretraining, and high parameter counts.</p>
<h2 id="the-attention-mechanism">
        
        
          The Attention Mechanism <a href="#the-attention-mechanism"></a>
</h2>
<p>The self-attention mechanism found in transformers is one of the key
design components that make transformer-based LLMs so successful.
However, transformers are not the first architecture to utilize
attention mechanisms.</p>
<p>Attention mechanisms were first developed in the context of image
recognition back in 2010, before being adopted to aid the translation of
long sentences in recurrent neural networks.
(ChapterÂ <a data-reference="ch16" data-reference-type="ref" href="../ch16">[ch16]</a> compares the attention mechanisms found
in recurrent neural networks and transformers in greater detail.)</p>
<p>The aforementioned attention mechanism is inspired by human vision,
focusing on specific parts of an image (foveal glimpses) at a time to
process information hierarchically and sequentially. In contrast, the
fundamental mechanism underlying transformers is a self-attention
mechanism used for sequence-to-sequence tasks, such as machine
translation and text generation. It allows each token in a sequence to
attend to all other tokens, thus providing context-aware representations
of each token.</p>
<p>What makes attention mechanisms so unique and useful? For the following
illustration, suppose we are using an encoder network on a fixed-length
representation of the input sequence or imageâthis can be a fully
connected, convolutional, or attention-based encoder.</p>
<p>In a transformer, the encoder uses self-attention mechanisms to compute
the importance of each input token relative to other tokens in the
sequence, allowing the model to focus on relevant parts of the input
sequence. Conceptually, attention mechanisms allow the transformers to
attend to different parts of a sequence or image. On the surface, this
sounds very similar to a fully connected layer where each input element
is connected via a weight with the input element in the next layer. In
attention mechanisms, the computation of the attention weights involves
comparing each input
element to all others. The attention weights obtained by this approach
are dynamic and input dependent. In contrast, the weights of a
convolutional or fully connected layer are fixed after training, as
illustrated in
FigureÂ <a data-reference="fig:ch08-fig01" data-reference-type="ref" href="#fig:ch08-fig01">1.1</a>.</p>
<figure id="fig:ch08-fig01">
<img src="../images/ch08-fig01.png">
<figcaption>The conceptual difference between model weights in fully
connected<br>
layers (top) and attention scores (bottom)</br></figcaption>
</img></figure>
<p>As the top part of
FigureÂ <a data-reference="fig:ch08-fig01" data-reference-type="ref" href="#fig:ch08-fig01">1.1</a> shows, once trained, the weights
of fully connected layers remain fixed regardless of the input. In
contrast, as shown at the bottom, self-attention weights change
depending on the inputs, even after a transformer is trained.</p>
<p>Attention mechanisms allow a neural network to selectively weigh the
importance of different input features, so the model can focus on the
mostrelevant parts of the input for a given task. This provides a
contextual understanding of each word or image token, allowing for more
nuanced interpretations, which is one of the aspects that can make
transformers work
so well.</p>
<h2 id="pretraining-via-self-supervised-learning">
        
        
          Pretraining via Self-Supervised Learning <a href="#pretraining-via-self-supervised-learning"></a>
</h2>
<p>Pretraining transformers via self-supervised learning on large,
unlabeled datasets is another key factor in the success of transformers.
During pretraining, the transformer model is trained to predict missing
words in a
sentence or the next sentence in a document, for example. By learning to
predict these missing words or the next sentence, the model is forced to
learn general representations of language that can be fine-tuned for a
wide range of downstream tasks.</p>
<p>While unsupervised pretraining has been highly effective for natural
language processing tasks, its effectiveness for computer vision tasks
is still an active area of research. (Refer to
ChapterÂ <a data-reference="ch02" data-reference-type="ref" href="../ch02">[ch02]</a> for a more detailed discussion of
self-supervised learning.)</p>
<h2 id="large-numbers-of-parameters">
        
        
          Large Numbers of Parameters <a href="#large-numbers-of-parameters"></a>
</h2>
<p>One noteworthy characteristic of transformers is their large model
sizes. For example, the popular 2020 GPT-3 model consists of 175 billion
trainable parameters, while other transformers, such as switch
transformers, have trillions of parameters.</p>
<p>The scale and number of trainable parameters of transformers are
essential factors in their modeling performance, particularly for
large-scale natural language processing tasks. For instance, linear
scaling laws suggest that the training loss decreases proportionally
with an increase in model size, so a doubling of the model size can
halve the training loss.</p>
<p>This, in turn, can lead to better performance on the downstream target
task. However, it is essential to scale the model size and the number of
training tokens equally. This means the number of training tokens
should
be doubled for every doubling of model size.</p>
<p>Since labeled data is limited, utilizing large amounts of data during
unsupervised pretraining is vital.</p>
<p>To summarize, large model sizes and large datasets are critical factors
in transformersâ success. Additionally, using self-supervised learning,
the ability to pretrain transformers is closely tied to using large
model sizes and large datasets. This combination has been critical in
enabling the success of transformers in a wide range of natural language
processing tasks.</p>
<h2 id="easy-parallelization">
        
        
          Easy Parallelization <a href="#easy-parallelization"></a>
</h2>
<p>Traininglargemodelsonlargedatasetsrequiresvastcomputational
resources, and itâs key that the computations can be parallelized to
utilize these resources.</p>
<p>Fortunately,transformersareeasytoparallelizesincetheytakea
fixed-length sequence of word or image tokens as input. For instance,
the self-attention mechanism used in most transformer architectures
involves computing the weighted sum between a pair of input elements.
Furthermore, these pair-wise token comparisons can be computed
independently,
as illustrated in
FigureÂ <a data-reference="fig:ch08-fig02" data-reference-type="ref" href="#fig:ch08-fig02">1.2</a>, making the self-attention
mechanism relatively easy to parallelize across different GPU cores.</p>
<figure id="fig:ch08-fig02">
<img src="../images/ch08-fig02.png">
<figcaption>A simplified self-attention mechanism without<br>
weight parameters</br></figcaption>
</img></figure>
<p>In addition, the individual weight matrices used in the self-attention
mechanism (not shown in
FigureÂ <a data-reference="fig:ch08-fig02" data-reference-type="ref" href="#fig:ch08-fig02">1.2</a>) can be distributed across
different machines for distributed and parallel computing.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>8-1. As discussed in this chapter,
self-attention is easily parallelizable, yet transformers are considered
computationally expensive due to self-attention. How can we explain this
contradiction?</p>
<p>8-2. Since self-attention
scores represent importance weights for the various input elements, can
we consider self-attention to be a form of feature selection?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>An example of an attention mechanism in the context of image rec-
Â ognition: Hugo Larochelle and Geoffrey Hinton, âLearning to
Combine Foveal Glimpses with a Third-Order Boltzmann Machineâ
(2010), <a href="https://dl.acm.org/doi/10.5555/2997189.2997328">https://dl.acm.org/doi/10.5555/2997189.2997328</a>.</p>
</li>
<li>
<p>The paper introducing the self-attention mechanism with the original
transformer architecture: Ashish Vaswani et al., âAttention Is All You
Needâ (2017), <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
</li>
<li>
<p>Transformers can have trillions of parameters: William Fedus, Barret
Zoph, and Noam Shazeer, âSwitch Transformers: Scaling to Trillion
Parameter Models with Simple and Efficient Sparsityâ (2021),
<a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a>.</p>
</li>
<li>
<p>Linear scaling laws suggest that training loss decreases
proportionally with an increase in model size: Jared Kaplan et al.,
âScaling Laws for Neural Language Modelsâ (2020),
<a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a>.</p>
</li>
<li>
<p>Research suggests that in transformer-based language models, the
training tokens should be doubled for every doubling of model size:
Jordan Hoffmann et al., âTraining Compute-Optimal Large Language
Modelsâ (2022), <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>.</p>
</li>
<li>
<p>Formoreabouttheweightsusedinself-attentionandcross-attention
mechanisms, check out my blog post: âUnderstanding and Coding the
Self-Attention Mechanism of Large Language Models from Scratchâ at
<a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
