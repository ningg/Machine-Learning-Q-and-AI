<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 22
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch22/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 22" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 22</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 22" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch22/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch22/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 22" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch22/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 22" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-22-speeding-up-inference">
        
        
          Chapter 22: Speeding Up Inference <a href="#chapter-22-speeding-up-inference"></a>
</h1>
<p><span id="ch22" label="ch22"></span></p>
<p><strong>What are techniques to speed up model inference through optimization
without changing the model architecture or sacrificing accuracy?</strong></p>
<p>In machine learning and AI, <em>model inference</em> refers to making
predictions or generating outputs using a trained model. The main
general techniques for improving model performance during inference
include parallelization, vectorization, loop tiling, operator fusion,
and quantization, which are discussed in detail in the following
sections.</p>
<h2 id="parallelization">
        
        
          Parallelization <a href="#parallelization"></a>
</h2>
<p>One common way to achieve better parallelization during inference is to
run the model on a batch of samples rather than on a single sample at a
time. This is sometimes also referred to as <em>batched inference</em> and
assumes that we are receiving multiple input samples or user inputs
simultaneously or within a short time window, as illustrated in
FigureÂ <a data-reference="fig:ch22-fig01" data-reference-type="ref" href="#fig:ch22-fig01">1.1</a>.</p>
<figure id="fig:ch22-fig01">
<img src="../images/ch22-fig01.png">
<figcaption>Sequential inference and batched inference</figcaption>
</img></figure>
<p>FigureÂ <a data-reference="fig:ch22-fig01" data-reference-type="ref" href="#fig:ch22-fig01">1.1</a> shows sequential inference
processing one item at a time, which creates a bottleneck if there are
several samples waiting to be classified. In batched inference, the
model processes all four samples at the
same time.</p>
<h2 id="vectorization">
        
        
          Vectorization <a href="#vectorization"></a>
</h2>
<p><em>Vectorization</em> refers to performing operations on entire data
structures, such as arrays (tensors) or matrices, in a single step
rather than using iterative constructs like <code class="language-plaintext highlighter-rouge">for</code> loops. Using
vectorization, multiple operations from the loop are performed
simultaneously using single instruction, multiple data (SIMD)
processing, which is available on most modern CPUs.</p>
<p>This approach takes advantage of the low-level optimizations in many
computing systems and often results in significant speedups. For
example, it might rely on BLAS.</p>
<p><em>BLAS</em> (which is short for <em>Basic Linear Algebra Subprograms</em>) is a
specification that prescribes a set of low-level routines for performing
common linear algebra operations such as vector addition, scalar
multiplication, dot products, matrix multiplication, and others. Many
array and deep learning libraries like NumPy and PyTorch use BLAS under
the hood.</p>
<p>To illustrate vectorization with an example, suppose we wanted to
compute the dot product between two vectors. The non-vectorized way of
doing this would be to use a <code class="language-plaintext highlighter-rouge">for</code> loop, iterating over each element of
the array one by one. However, this can be quite slow, especially for
large arrays. With vectorization, you can perform the dot product
operation on the entire array at once, as shown in
FigureÂ <a data-reference="fig:ch22-fig02" data-reference-type="ref" href="#fig:ch22-fig02">1.2</a>.</p>
<figure id="fig:ch22-fig02">
<img src="../images/ch22-fig02.png">
<figcaption>A classic loop versus a vectorized dot<br>
product computation in Python</br></figcaption>
</img></figure>
<p>In the context of linear algebra or deep learning frameworks like
TensorFlow and PyTorch, vectorization is typically done automatically.
This is because these frameworks are designed to work with
multidimensional arrays (also known as <em>tensors</em>), and their operations
are inherently vectorized. This means that when you perform functions
using these frameworks, you automatically leverage the power of
vectorization, resulting in faster and more efficient computations.</p>
<h2 id="loop-tiling">
        
        
          Loop Tiling <a href="#loop-tiling"></a>
</h2>
<p><em>Loop tiling</em> (also often referred to as <em>loop nest optimization</em>) is an
advanced optimization technique to enhance data locality by breaking
down a loopâs iteration space into smaller chunks or âtiles.â This
ensures that once data is loaded into cache, all possible computations
are performed on it before the cache is cleared.</p>
<p>FigureÂ <a data-reference="fig:ch22-fig03" data-reference-type="ref" href="#fig:ch22-fig03">1.3</a> illustrates the concept of loop
tiling for accessing elements in a two-dimensional array. In a regular
<code class="language-plaintext highlighter-rouge">for</code> loop, we iterate over columns and rows one element at a time,
whereas in loop tiling, we subdivide the array into smaller tiles.</p>
<figure id="fig:ch22-fig03">
<img src="../images/ch22-fig03.png">
<figcaption>Loop tiling in a two-dimensional array</figcaption>
</img></figure>
<p>Note that in languages such as Python, we donât usually perform loop
tiling, because Python and many other high-level languages do not allow
control over cache memory like lower-level languages such as C and C++
do. These kinds of optimizations are often handled by underlying
libraries like NumPy and PyTorch when performing operations on large
arrays.</p>
<h2 id="operator-fusion">
        
        
          Operator Fusion <a href="#operator-fusion"></a>
</h2>
<p><em>Operator fusion</em>, sometimes called <em>loop fusion</em>, is an optimization technique that
combines multiple loops into a single loop. This is illustrated in
FigureÂ <a data-reference="fig:ch22-fig04" data-reference-type="ref" href="#fig:ch22-fig04">1.4</a>, where two separate loops to
calculate the sum and the product of an array of numbers are fused into
a single loop.</p>
<figure id="fig:ch22-fig04">
<img src="../images/ch22-fig04.png">
<figcaption>Fusing two loops (left) into one (right)</figcaption>
</img></figure>
<p>Operator fusion can improve the performance of a model by reducing the
overhead of loop control, decreasing memory access times by improving
cache performance, and possibly enabling further optimizations through
vectorization. You might think this behavior of vectorization would be
incompatible with loop tiling, in which we break a <code class="language-plaintext highlighter-rouge">for</code> loop into
multiple loops. However, these techniques are actually complementary,
used for different optimizations, and applicable in different
situations. Operator fusion is about reducing the total number of loop
iterations and improving data locality when the entire data fits into
cache. Loop tiling is about improving cache utilization when dealing
with larger multidimensional arrays that do not fit into cache.</p>
<p>Related to operator fusion is the concept of <em>reparameterization</em>, which
can often also be used to simplify multiple operations into one. Popular
examples include training a network with multibranch architectures that
are reparameterized into single-stream architectures during inference.
This reparameterization approach differs from traditional operator
fusion in that it does not merge multiple operations into a single
operation. Instead, it rearranges the operations in the network to
create a more efficient architecture for inference. In the so-called
RepVGG architecture, for example, each branch during training consists
of a series of convolutions. Once training is complete, the model is
reparameterized into a single sequence of convolutions.</p>
<h2 id="quantization">
        
        
          Quantization <a href="#quantization"></a>
</h2>
<p><em>Quantization</em> reduces the computational and storage requirements of
machine learning models, particularly deep neural networks. This
technique
involves converting the floating-point numbers (technically discrete but
representing continuous values within a specific range) for implementing
weights and biases in a trained neural network to more discrete,
lower-
Â precision representations such as integers. Using less precision
reduces the model size and makes it quicker to execute, which can lead
to significant improvements in speed and hardware efficiency during
inference.</p>
<p>In the realm of deep learning, it has become increasingly common to
quantize trained models down to 8-bit and 4-bit integers. These
techniques are especially prevalent in the deployment of large language
models.</p>
<p>There are two main categories of quantization. In <em>post-training
quantization</em>, the model is first trained normally with full-precision
weights, which are then quantized after training. <em>Quantization-aware
training</em>, on the other hand, introduces the quantization step during
the training process. This allows the model to learn to compensate for
the effects of quantization, which can help maintain the modelâs
accuracy.</p>
<p>However, itâs important to note that quantization can occasionally lead
to a reduction in model accuracy. Since this chapter focuses on
techniques to speed up model inference <em>without</em> sacrificing accuracy,
quantization is not as good a fit for this chapter as the previous
categories.</p>
<div class="note">

Other techniques to improve inference speeds include knowledge
distillation and pruning, discussed in
ChapterÂ <a data-reference="ch06" data-reference-type="ref" href="../ch06">[ch06]</a>. However, these techniques affect the
model architecture, resulting in smaller models, so they are out of
scope for this chapterâs question.

</div>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>22-1.
ChapterÂ <a data-reference="ch07" data-reference-type="ref" href="../ch07">[ch07]</a> covered several multi-GPU training
paradigms to speed up
modeltraining.UsingmultipleGPUscan,intheory,alsospeedupmodel inference.
However, in reality, this approach is often not the most efficient or
most practical option. Why is that?</p>
<p>22-2.
Vectorization and loop tiling are two strategies for optimizing
operations that involve accessing array elements. What would be the
ideal situation in which to use each?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>The official BLAS website: <a href="https://www.netlib.org/blas/">https://www.netlib.org/blas/</a>.</p>
</li>
<li>
<p>The paper that proposed loop tiling: Michael Wolfe, âMore Iteration
Space Tilingâ (1989),
<a href="https://dl.acm.org/doi/abs/10.1145/76263.76337">https://dl.acm.org/doi/abs/10.1145/76263.76337</a>.</p>
</li>
<li>
<p>RepVGG CNN architecture merging operations in inference mode: Xiaohan
Ding et al., âRepVGG: Making VGG-style ConvNets Great Againâ (2021),
<a href="https://arxiv.org/abs/2101.03697">https://arxiv.org/abs/2101.03697</a>.</p>
</li>
<li>
<p>A new method for quantizing the weights in large language mod-
Â els downto8-bitintegerrepresentations:TimDettmersetal.,
âLLM.int8(): 8-bit Matrix Multiplication for Transformers at Scaleâ
(2022), <a href="https://arxiv.org/abs/2208.07339">https://arxiv.org/abs/2208.07339</a>.</p>
</li>
<li>
<p>A new method for quantizing the weights in LLMs farther down to 4-bit
integers: Elias Frantar et al., âGPTQ: Accurate Post-Training
Quantization for Generative Pre-trained Transformersâ (2022),
<a href="https://arxiv.org/abs/2210.17323">https://arxiv.org/abs/2210.17323</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
