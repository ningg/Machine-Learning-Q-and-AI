<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 10
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch10/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 10" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 10</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 10" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch10/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch10/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 10" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch10/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 10" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-10-sources-of-randomness">
        
        
          Chapter 10: Sources of Randomness <a href="#chapter-10-sources-of-randomness"></a>
</h1>
<p><span id="ch10" label="ch10"></span></p>
<p><strong>What are the common sources of randomness when training deep neural
networks that can cause non-reproducible behavior during training and
inference?</strong></p>
<p>When training or using machine learning models such as deep neural
networks, several sources of randomness can lead to different results
every time we train or run these models, even though we use the same
overall settings. Some of these effects are accidental and some are
intended. The following sections categorize and discuss these various
sources of randomness.</p>
<p>Optional hands-on examples for most of these categories are provided in
the <em>supplementary/q10-random-sources</em> subfolder at
<a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a>.</p>
<h2 id="model-weight-initialization">
        
        
          Model Weight Initialization <a href="#model-weight-initialization"></a>
</h2>
<p>All common deep neural network frameworks, including TensorFlow and
PyTorch, randomly initialize the weights and bias units at each layer by
default. This means that the final model will be different every time we
start the training. The reason these trained models will differ when we
start with different random weights is the nonconvex nature of the loss,
as illustrated in
FigureÂ <a data-reference="fig:ch10-fig01" data-reference-type="ref" href="#fig:ch10-fig01">1.1</a>. As the figure shows, the loss
will converge to different local minima depending on where the initial
starting weights are located.</p>
<figure id="fig:ch10-fig01">
<img src="../images/ch10-fig01.png">
<figcaption>Different starting weights can lead to<br>
different final weights.</br></figcaption>
</img></figure>
<p>In practice, it is therefore recommended to run the training (if the
computational resources permit) at least a handful of times; unlucky
initial weights can sometimes cause the model not to converge or to
converge to a local minimum corresponding to poorer predictive accuracy.</p>
<p>However, we can make the random weight initialization deterministic by
seeding the random generator. For instance, if we set the seed to a
specific value like 123, the weights will still initialize with small
random values. Nonetheless, the neural network will consistently
initialize with the same small random weights, enabling accurate
reproduction of results.</p>
<h2 id="dataset-sampling-and-shuffling">
        
        
          Dataset Sampling and Shuffling <a href="#dataset-sampling-and-shuffling"></a>
</h2>
<p>When we train and evaluate machine learning models, we usually start by
dividing a dataset into training and test sets. This requires random
sampling since we have to decide which examples we put into a training
set and which examples we put into a test set.</p>
<p>In practice, we often use model evaluation techniques such as <em>k</em>-fold
cross-validation or holdout validation. In holdout validation, we split
the training set into training, validation, and test datasets, which are
also sampling procedures influenced by randomness. Similarly, unless we
use a fixed random seed, we get a different model each time we partition
the dataset or tune or evaluate the model using <em>k</em>-fold
cross-validation since the training partitions will differ.</p>
<h2 id="nondeterministic-algorithms">
        
        
          Nondeterministic Algorithms <a href="#nondeterministic-algorithms"></a>
</h2>
<p>We may include random components and algorithms depending on the
architecture and hyperparameter choices. A popular example of this is
<em>dropout</em>.</p>
<p>Dropout works by randomly setting a fraction of a layerâs units to zero
during training, which helps the model learn more robust and generalized
representations. This âdropping outâ is typically applied at each
training iteration with a probability <em>p</em>, a hyperparameter that
controls the fraction of units dropped out. Typical values for <em>p</em> are
in the range of 0.2 to 0.8.</p>
<p>To illustrate this concept,
FigureÂ <a data-reference="fig:ch10-fig02" data-reference-type="ref" href="#fig:ch10-fig02">1.2</a> shows a small neural network
where dropout randomly drops a subset of the hidden layer nodes in each
forward pass during training.</p>
<figure id="fig:ch10-fig02">
<img src="../images/ch10-fig02.png">
<figcaption>In dropout, hidden nodes are intermittently and randomly
disabled<br>
during each forward pass in training.</br></figcaption>
</img></figure>
<p>To create reproducible training runs, we must seed the random gen-
Â erator before training with dropout (analogous to seeding the random
generator before initializing the model weights). During inference, we
need to disable dropout to guarantee deterministic results. Each deep
learning framework has a specific setting for that purposeâa PyTorch
example is included in the <em>supplementary/q10-random-sources</em> subfolder
at <a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a>.</p>
<h2 id="different-runtime-algorithms">
        
        
          Different Runtime Algorithms <a href="#different-runtime-algorithms"></a>
</h2>
<p>The most intuitive or simplest implementation of an algorithm or method
is not always the best one to use in practice. For example, when
training deep neural networks, we often use efficient alternatives and
approximations to gain speed and resource advantages during training and
inference.</p>
<p>A popular example is the convolution operation used in convolutional
neural networks. There are several possible ways to implement the
convolution operation:</p>
<p>The classic direct convolution
The common implementation of discrete convolution via an element-wise
product between the input and the window, followed by summing the result
to get a single number. (See
ChapterÂ <a data-reference="ch12" data-reference-type="ref" href="../ch12">[ch12]</a> for a discussion of the convolution
operation.)</p>
<p>FFT-based convolution
Uses fast Fourier transform (FFT) to convert the convolution into an
element-wise multiplication in the frequency domain.</p>
<p>Winograd-based convolution
An efficient algorithm for small filter sizes (like
3$<code class="language-plaintext highlighter-rouge">\times</code>$<!-- -->3) that reduces the number of multiplications
required for the convolution.</p>
<p>Different convolution algorithms have different trade-offs in terms of
memory usage, computational complexity, and speed. By default, libraries
such as the CUDA Deep Neural Network library (cuDNN), which are used in
PyTorch and TensorFlow, can choose different algorithms for performing
convolution operations when running deep neural networks on GPUs.
However, the deterministic algorithm choice has to be explicitly
enabled. In PyTorch, for example, this can be done by setting</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.use_deterministic_algorithms(True)
</code></pre></div></div>
<p>While these approximations yield similar results, subtle numerical
differences can accumulate during training and cause the training to
converge to slightly different local minima.</p>
<h2 id="hardware-and-drivers">
        
        
          Hardware and Drivers <a href="#hardware-and-drivers"></a>
</h2>
<p>Training deep neural networks on different hardware can also produce
different results due to small numeric differences, even when the same
algorithms are used and the same operations are executed. These
differences may sometimes be due to different numeric precision for
floating-point operations. However, small numeric differences may also
arise due to hardware and software optimization, even at the same
precision.</p>
<p>For instance, different hardware platforms may have specialized
optimizations or libraries that can slightly alter the behavior of deep
learning algorithms. To give one example of how different GPUs can
produce different modeling results, the following is a quotation from
the official NVIDIA documentation: âAcross different architectures, no
cuDNN routines guarantee bit-wise reproducibility. For example, there is
no guarantee of bit-wise reproducibility when comparing the same routine
run on NVIDIA Voltaâ¢ and NVIDIA Turingâ¢ [. . .] and NVIDIA Ampere
architecture.â</p>
<h2 id="randomness-and-generative-ai">
        
        
          Randomness and Generative AI <a href="#randomness-and-generative-ai"></a>
</h2>
<p>Besides the various sources of randomness mentioned earlier, certain
models may also exhibit random behavior during inference that we can
think of as ârandomness by design.â For instance, generative image and
language models may create different results for identical prompts to
produce a diverse sample of results. For image models, this is often so
that users canselect the most accurate and aesthetically pleasing image.
For language models, this is often to vary the responses, for example,
in chat agents, to avoid repetition.</p>
<p>The intended randomness in generative image models during inference is
often due to sampling different noise values at each step of the reverse
process. In diffusion models, a noise schedule defines the noise
variance added at each step of the diffusion process.</p>
<p>Autoregressive LLMs like GPT tend to create different outputs for the
same input prompt (GPT will be discussed at greater length in
ChaptersÂ <a data-reference="ch14" data-reference-type="ref" href="../ch14">[ch14]</a>
andÂ <a data-reference="ch17" data-reference-type="ref" href="../ch17">[ch17]</a>). The ChatGPT user interface even has a
Regenerate Response button for that purpose. The ability to generate
different results is due to the sampling strategies these models employ.
Techniques such as top-<em>k</em> sampling, nucleus sampling, and temperature
scaling influence the modelâs output by controlling the degree of
randomness. This is a feature, not a bug, since it allows for diverse
responses and prevents the model from producing overly deterministic or
repetitive outputs. (See
ChapterÂ <a data-reference="ch09" data-reference-type="ref" href="../ch09">[ch09]</a> for a more in-depth overview of
generative AI and deep learning models; see
ChapterÂ <a data-reference="ch17" data-reference-type="ref" href="../ch17">[ch17]</a> for more detail on autoregressive
LLMs.)</p>
<p><em>Top-<span class="upright">k</span> sampling</em>, illustrated in
FigureÂ <a data-reference="fig:ch10-fig03" data-reference-type="ref" href="#fig:ch10-fig03">1.3</a>, works by sampling tokens from
the top <em>k</em> most probable candidates at each step of the next-word
generation process.</p>
<figure id="fig:ch10-fig03">
<img src="../images/ch10-fig03.png">
<figcaption>Top-<span class="upright">k</span> sampling</figcaption>
</img></figure>
<p>Given an input prompt, the language model produces a probability
distribution over the entire vocabulary (the candidate words) for the
next token. Each token in the vocabulary is assigned a probability based
on the modelâs understanding of the context. The selected top-<em>k</em> tokens
are then renormalized so that the probabilities sum to 1. Finally, a
token is sampled from the renormalized top-<em>k</em> probability distribution
and is appended to the input prompt. This process is repeated for the
desired length of the generated text or until a stop condition is met.</p>
<p><em>Nucleus sampling</em> (also known as <em>top-<span class="upright">p</span>
sampling</em>), illustrated in
FigureÂ <a data-reference="fig:ch10-fig04" data-reference-type="ref" href="#fig:ch10-fig04">1.4</a>, is an alternative to top-<em>k</em>
sampling.</p>
<figure id="fig:ch10-fig04">
<img src="../images/ch10-fig04.png">
<figcaption>Nucleus sampling</figcaption>
</img></figure>
<p>Similar to top-<em>k</em> sampling, the goal of nucleus sampling is to balance
diversity and coherence in the output. However, nucleus and top-<em>k</em>
sampling differ in how to select the candidate tokens for sampling at
each step of the generation process. Top-<em>k</em> sampling selects the <em>k</em>
most probable tokens from the probability distribution produced by the
language model, regardless of their probabilities. The value of <em>k</em>
remains fixed throughout the generation process. Nucleus sampling, on
the other hand, selects tokens based on a probability threshold <em>p</em>, as
shown in FigureÂ <a data-reference="fig:ch10-fig04" data-reference-type="ref" href="#fig:ch10-fig04">1.4</a>. It then accumulates the most
probable tokens in descending order until their cumulative probability
meets or exceeds the threshold <em>p</em>. In contrast to top-<em>k</em> sampling, the
size of the candidate set (nucleus) can vary at each step.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>10-1. Suppose we train a neural network with
top-<em>k</em> or nucleus sampling where <em>k</em> and <em>p</em> are hyperparameter
choices. Can we make the model behave deterministically during inference
without changing the code?</p>
<p>10-2. In what
scenarios might random dropout behavior during inference be desired?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>For more about different data sampling and model evaluation
techniques, see my article: âModel Evaluation, Model Selection, and
Algorithm Selection in Machine Learningâ (2018),
<a href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</a>.</p>
</li>
<li>
<p>The paper that originally proposed the dropout technique: Nitish
Srivastavaetal.,âDropout:ASimpleWaytoPreventNeuralNet-
Â works from Overfittingâ (2014),
<a href="https://jmlr.org/papers/v15/srivastava14a.html"><em>https://jmlr.org/papers/v15/sriva</em></a>
<a href="https://jmlr.org/papers/v15/srivastava14a.html"><em>stava14a.html</em></a>.</p>
</li>
<li>
<p>A detailed paper on FFT-based convolution: Lu Chi, Borui Jiang, and
Yadong Mu, âFast Fourier Convolutionâ (2020),
<a href="https://dl.acm.org/doi/abs/10.5555/3495724.3496100">https://dl.acm.org/doi/abs/10.5555/3495724.3496100</a>.</p>
</li>
<li>
<p>Details on Winograd-based convolution: Syed Asad Alam et al.,
âWinograd Convolution for Deep Neural Networks: Efficient Point
Selectionâ (2022), <a href="https://arxiv.org/abs/2201.10369">https://arxiv.org/abs/2201.10369</a>.</p>
</li>
<li>
<p>More information about the deterministic algorithm settings in
PyTorch:
<a href="https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html">https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html</a>.</p>
</li>
<li>
<p>For details on the deterministic behavior of NVIDIA graphics cards,
see the âReproducibilityâ section of the official NVIDIA
documentation:
<a href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility">https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
