<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 16
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch16/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 16" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 16</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 16" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch16/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch16/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 16" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch16/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 16" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-16-self-attention">
        
        
          Chapter 16: Self-Attention <a href="#chapter-16-self-attention"></a>
</h1>
<p><span id="ch16" label="ch16"></span></p>
<p><strong>Where does self-attention get its name, and how is it different from
previously developed attention mechanisms?</strong></p>
<p><em>Self-attention</em> enables a neural network to refer to other portions of
the input while focusing on a particular segment, essentially allowing
each part the ability to âattendâ to the whole input. The original
attention mechanism developed for recurrent neural networks (RNNs) is
applied between two
different sequences: the encoder and the decoder embeddings. Since the
attention mechanisms used in transformer-based large language models is
designed to work on all elements of the same set, it is known as
<em>self</em>-attention.</p>
<p>This chapter first discusses an earlier attention mechanism developed
for RNNs, the Bahdanau mechanism, in order to illustrate the motivation
behind developing attention mechanism. We then compare the Bahdanau
mechanism to the self-attention mechanism prevalent in transformer
architectures today.</p>
<h2 id="attention-in-rnns">
        
        
          Attention in RNNs <a href="#attention-in-rnns"></a>
</h2>
<p>One example of an attention mechanism used in RNNs to handle long
sequences is <em>Bahdanau attention</em>. Bahdanau attention was developed to
make machine learning models, particularly those used in translating
languages, better at understanding long sentences. Before this type of
attention, the whole input (such as a sentence in English) was squashed
into a single chunk of information, and important details could get
lost,
especially if the sentence was long.</p>
<p>Tounderstandthedifferencebetweenregularattentionandself-
attention,letâsbeginwiththeillustrationoftheBahdanauattention
mechanisminFigureÂ <a data-reference="fig:ch16-fig01" data-reference-type="ref" href="#fig:ch16-fig01">1.1</a>.</p>
<figure id="fig:ch16-fig01">
<img src="../images/ch16-fig01.png">
<figcaption>The Bahdanau mechanism uses a<br>
separate RNN to compute attention weights.</br></figcaption>
</img></figure>
<p>In FigureÂ <a data-reference="fig:ch16-fig01" data-reference-type="ref" href="#fig:ch16-fig01">1.1</a>, the \(\alpha\) values represent
the attention weights for the second sequence element and each other
element in the sequence from 1
to <em>T</em>. Furthermore, this original attention mechanism involves two
RNNs. The RNN at the bottom, computing the attention weights, represents
the encoder, while the RNN at the top, producing the output sequence, is
a
decoder.</p>
<p>In short, the original attention mechanism developed for RNNs is applied
between two different sequences: the encoder and decoder embeddings. For
each generated output sequence element, the decoder RNN at the top is
based on a hidden state plus a context vector generated by the encoder.
This context vector involves <em>all</em> elements of the input sequence and is
a weighted sum of all input elements where the attention scores
(\(\alpha\)âs) represent the weighting coefficients. This allows the
decoder to access all input sequence elements (the context) at each
step. The key idea is that the attention weights (and context) may
differ and change dynamically at each step.</p>
<p>The motivation behind this complicated encoder-decoder design is that we
cannot translate sentences word by word. This would result in
grammatically incorrect outputs, as illustrated by the RNN architecture
(a) in FigureÂ <a data-reference="fig:ch16-fig02" data-reference-type="ref" href="#fig:ch16-fig02">[fig:ch16-fig02]</a>.</p>
<div class="figurewide">
<img alt="image" src="../images/ch16-fig02.png" style="width:5.625in">
</img></div>
<p>FigureÂ <a data-reference="fig:ch16-fig02" data-reference-type="ref" href="#fig:ch16-fig02">[fig:ch16-fig02]</a> shows two different
sequence-to-sequence RNN designs for sentence translation.
FigureÂ <a data-reference="fig:ch16-fig02" data-reference-type="ref" href="#fig:ch16-fig02">[fig:ch16-fig02]</a>(a) represents a
regular sequence-to-sequence RNN that may be used to translate a
sentence from German to English word by word.
FigureÂ <a data-reference="fig:ch16-fig02" data-reference-type="ref" href="#fig:ch16-fig02">[fig:ch16-fig02]</a>(b) depicts an
encoder-decoder RNN that first reads the whole sentence before
translating it.</p>
<p>RNN architecture (a) is best suited for time series tasks in which we
want to make one prediction at a time, such as predicting a given stock
price day by day. For tasks like language translation, we typically opt
for an encoder-decoder RNN, as in architecture (b) in
FigureÂ <a data-reference="fig:ch16-fig02" data-reference-type="ref" href="#fig:ch16-fig02">[fig:ch16-fig02]</a>. Here, the RNN
encodes the input sentence, stores it in an intermediate hidden
representation, and generates the output sentence. However, this creates
a bottleneck where the RNN has to memorize the whole input sentence via
a single hidden state, which does not work well for longer sequences.</p>
<p>The bottleneck depicted in architecture (b) prompted the Bahdanau
attention mechanismâs original design, allowing the decoder to access
all elements in the input sentence at each time step. The attention
scores also give different weights to the different input elements
depending on the current word that the decoder generates. For example,
when generating the word <em>help</em> in the output sequence, the word
<em>helfen</em> in the German input sentence may get a large attention weight,
as itâs highly relevant in this context.</p>
<h2 id="the-self-attention-mechanism">
        
        
          The Self-Attention Mechanism <a href="#the-self-attention-mechanism"></a>
</h2>
<p>The Bahdanau attention mechanism relies on a somewhat complicated
encoder-decoder design to model long-term dependencies in sequence-
Â to-sequence language modeling tasks. Approximately three years after
the
Bahdanau mechanism, researchers worked on simplifying sequence-to-
Â sequence modeling architectures by asking whether the RNN backbone
was even needed to achieve good language translation performance. This
led to the design of the original transformer architecture and
self-attention mechanism.</p>
<p>In self-attention, the attention mechanism is applied between all
elements in the same sequence (as opposed to involving two sequences),
as depicted in the simplified attention mechanism in
FigureÂ <a data-reference="fig:ch16-fig03" data-reference-type="ref" href="#fig:ch16-fig03">1.2</a>. Similar to the attention
mechanism for RNNs, the context vector is an attention-weighted sum over
the input sequence elements.</p>
<figure id="fig:ch16-fig03">
<img src="../images/ch16-fig03.png">
<figcaption>A simple self-attention mechanism without weight
matrices</figcaption>
</img></figure>
<p>While FigureÂ <a data-reference="fig:ch16-fig03" data-reference-type="ref" href="#fig:ch16-fig03">1.2</a> doesnât include weight matrices,
the self-attention mechanism used in transformers typically involves
multiple weight matrices to compute the attention weights.</p>
<p>This chapter laid the groundwork for understanding the inner workings of
transformer models and the attention mechanism. The next chapter covers
the different types of transformer architectures in more detail.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>16-1. Considering that self-attention
compares each sequence element with itself, what is the time and memory
complexity of self-attention?</p>
<p>16-2. We
discussed self-attention in the context of natural language processing.
Could this mechanism be useful for computer vision applications
as well?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>The paper introducing the original self-attention mechanism, also
known as <em>scaled dot-product</em> attention: Ashish Vaswani et al.,
âAttention Is All You Needâ (2017),
<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
</li>
<li>
<p>The Bahdanau attention mechanism for RNNs: Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio, âNeural Machine Translation by Jointly
Learning to Align and Translateâ (2014),
<a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.</p>
</li>
<li>
<p>For more about the parameterized self-attention mechanism, check out
my blog post: âUnderstanding and Coding the Self-Attention Mechanism
of Large Language Models from Scratchâ at
<a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
