<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 25
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch25/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 25" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 25</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 25" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch25/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch25/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 25" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch25/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 25" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-25-confidence-intervals">
        
        
          Chapter 25: Confidence Intervals <a href="#chapter-25-confidence-intervals"></a>
</h1>
<p><span id="ch25" label="ch25"></span></p>
<p><strong>What are the different ways to construct confidence intervals for
machine learning classifiers?</strong></p>
<p>There are several ways to construct confidence intervals for machine
learning models, depending on the model type and the nature of your
data. For instance, some methods are computationally expensive when
working with deep neural networks and are thus more suitable to less
resource-intensive machine learning models. Others require larger
datasets to be
reliable.</p>
<p>The following are the most common methods for constructing confidence
intervals:</p>
<ul>
<li>
<p>Constructing normal approximation intervals based on a test set</p>
</li>
<li>
<p>Bootstrapping training sets</p>
</li>
<li>
<p>Bootstrapping the test set predictions</p>
</li>
<li>
<p>Confidence intervals from retraining models with different random
seeds</p>
</li>
</ul>
<p>Before reviewing these in greater depth, letâs briefly review the
definition and interpretation of confidence intervals.</p>
<h2 id="defining-confidence-intervals">
        
        
          Defining Confidence Intervals <a href="#defining-confidence-intervals"></a>
</h2>
<p>A <em>confidence interval</em> is a type of method to estimate an unknown
population parameter. A <em>population parameter</em> is a specific measure of
a statistical population, for example, a mean (average) value or
proportion. By âspecificâ measure, I mean there is a single, exact value
for that parameter for the entire population. Even though this value may
not be known and often needs to be estimated from a sample, it is a
fixed and definite characteristic of the population. A <em>statistical
population</em>, in turn, is the complete set of items or individuals we
study.</p>
<p>In a machine learning context, the population could be considered the
entire possible set of instances or data points that the model may
encounter, and the parameter we are often most interested in is the true
generalization accuracy of our model on this population.</p>
<p>The accuracy we measure on the test set estimates the true
generalization accuracy. However, itâs subject to random error due to
the specific
sample of test instances we happened to use. This is where the concept
of
a confidence interval comes in. A 95 percent confidence interval for the
generalization accuracy gives us a range in which we can be reasonably
sure that the true generalization accuracy lies.</p>
<p>Forinstance,ifwetake100differentdatasamplesandcomputea
95 percent confidence interval for each sample, approximately 95 of
the
100 confidence intervals will contain the true population value (such
as
the generalization accuracy), as illustrated in
FigureÂ <a data-reference="fig:ch25-fig01" data-reference-type="ref" href="#fig:ch25-fig01">[fig:ch25-fig01]</a>.</p>
<div class="figurewide">
<img alt="image" src="../images/ch25-fig01.png" style="width:5.625in">
</img></div>
<p>More concretely, if we were to draw 100 different representative test
sets from the population (for instance, the entire possible set of
instances that the model may encounter) and compute the 95 percent
confidence interval for the generalization accuracy from each test set,
we would expect about
95 of these intervals to contain the true generalization accuracy.</p>
<p>We can display confidence intervals in several ways. It is common to use
a bar plot representation where the top of the bar represents the
parameter value (for example, model accuracy) and the whiskers denote
the upper andlower levels of the confidence interval (left chart of
FigureÂ <a data-reference="fig:ch25-fig02" data-reference-type="ref" href="#fig:ch25-fig02">1.1</a>). Alternatively, the confidence
intervals can be shown without bars, as in the right chart of
FigureÂ <a data-reference="fig:ch25-fig02" data-reference-type="ref" href="#fig:ch25-fig02">1.1</a>.</p>
<figure id="fig:ch25-fig02">
<img src="../images/ch25-fig02.png">
<figcaption>Two common plotting variants to illustrate confidence
intervals</figcaption>
</img></figure>
<p>This visualization is functionally useful in a number of ways. For
instance, when confidence intervals for two model performances do
<em>not</em>
overlap, itâs a strong visual indicator that the performances are
signifi-
Â cantly different. Take the example of statistical significance tests,
such as t-tests: if two 95 percent confidence intervals do not overlap,
it strongly
suggests that the difference between the two measurements is
statistically significant at the 0.05 level.</p>
<p>On the other hand, if two 95 percent confidence intervals overlap, we
cannot automatically conclude that thereâs no significant difference
between the two measurements. Even when confidence intervals overlap,
there can still be a statistically significant difference.</p>
<p>Alternatively, to provide more detailed information about the exact
quantities, we can use a table view to express the confidence intervals.
The two common notations are summarized in
TableÂ <a data-reference="confidence-intervals" data-reference-type="ref" href="#confidence-intervals">1.1</a>.</p>
<div id="confidence-intervals">

|     |                      |       |       |     |
|-----|----------------------|-------|-------|-----|
| 1   | 89.1% $$\pm$$ 1.7%   | . . . | . . . |     |
| 2   | 79.5% $$\pm$$ 2.2%   | . . . | . . . |     |
| 3   | 95.2% $$\pm$$ 1.6%   | . . . | . . . |     |
|     |                      |       |       |     |
| 1   | 89.1% (87.4%, 90.8%) | . . . | . . . |     |
| 2   | 79.5% (77.3%, 81.7%) | . . . | . . . |     |
| 3   | 95.2% (93.6%, 96.8%) | . . . | . . . |     |

Confidence Intervals

</div>
<p>The \(\pm\) notation is often preferred if the confidence interval is
<em>symmetric</em>, meaning the upper and lower endpoints are equidistant from
the estimated parameter. Alternatively, the lower and upper confidence
intervals can be written explicitly.</p>
<h2 id="the-methods">
        
        
          The Methods <a href="#the-methods"></a>
</h2>
<p>The following sections describe the four most common methods of
constructing confidence intervals.</p>
<h3 id="method-1-normal-approximation-intervals">
        
        
          Method 1: Normal Approximation Intervals <a href="#method-1-normal-approximation-intervals"></a>
</h3>
<p>The normal approximation interval involves generating the confidence
interval from a single train-test split. It is often considered the
simplest and most traditional method for computing confidence intervals.
This approach is especially appealing in the realm of deep learning,
where training models is computationally costly. Itâs also desirable
when we are interested in evaluating a specific model, instead of models
trained on various data partitions like in <em>k</em>-fold cross-validation.</p>
<p>How does it work? In short, the formula for calculating the confidence
interval for a predicted parameter (for example, the sample mean,
denoted as \(\bar{\text{\emph{x}}})\), assuming a normal distribution,
is expressed as \(\bar{\text{\emph{x}}}\) \(\pm\) <em>z</em> \(\times\) <em>SE</em>.</p>
<p>In this formula, <em>z</em> represents the <em>z</em>-score, which indicates a
particular valueâs number of standard deviations from the mean in a
standard normal distribution. <em>SE</em> represents the standard error of the
predicted parameter (in this case, the sample mean).</p>
<div class="note">

Most readers will be familiar with <span class="upright">z</span>-score
tables that are usually found in the back of introductory statistics
textbooks. However, a more convenient and preferred way to obtain
<span class="upright">z</span>-scores is to use functions like SciPyâs
`stats.zscore` function, which computes the
<span class="upright">z</span>-scores for given confidence levels.

</div>
<p>For our scenario, the sample mean, denoted as \(\bar{\text{\emph{x}}}\),
corresponds to the test set accuracy, ACC<sub>test</sub>, a measure of
successful predictions in the context of a binomial proportion
confidence interval.</p>
<p>The standard error can be calculated under a normal approximation as
follows:</p>
<pre><code class="language-math">\text{SE} = \sqrt{ \frac{1}{n} \text{ACC}_{\text{test}}\left(1- \text{ACC}_{\text{test}}\right)}
</code></pre>
<p>In this equation, <em>n</em> signifies the size of the test set. Substituting
the standard error back into the previous formula, we obtain the
following:</p>
<pre><code class="language-math">\text{ACC}_{\text{test}} \pm z \sqrt{\frac{1}{n} \text{ACC}_{\text{test}}\left(1- \text{ACC}_{\text{test}}\right)}
</code></pre>
<p>Additional code examples to implement this method can also be found in
the <em>supplementary/q25_confidence-intervals</em> subfolder in the
supplementary code repository at
<a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a>. While the normal
approximation interval method is very popular due to its simplicity, it
has some downsides. First, the normal approximation may not always be
accurate, especially for small sample sizes or for data that is not
normally distributed. In such cases, other methods of computing
confidence intervals may be more accurate. Second, using a single
train-test split does not provide information about the variability of
the model performance across different splits of the data. This can be
an issue if the performance is highly dependent on the specific split
used, which may be the case if the dataset is small or if there is a
high degree of variability in the data.</p>
<h3 id="method-2-bootstrapping-training-sets">
        
        
          Method 2: Bootstrapping Training Sets <a href="#method-2-bootstrapping-training-sets"></a>
</h3>
<p>Confidence intervals serve as a tool for approximating unknown
parameters. However, when we are restricted to just one estimate, such
as the accuracy derived from a single test set, we must make certain
assumptions to make this work. For example, when we used the normal
approximation interval described in the previous section, we assumed
normally distributed data, which may or may not hold.</p>
<p>In a perfect scenario, we would have more insight into our test set
sample distribution. However, this would require access to many
independent test datasets, which is typically not feasible. A workaround
is the bootstrap method, which resamples existing data to estimate the
sampling distribution.</p>
<div class="note">

In practice, when the test set is large enough, the normal distribution
approximation will hold, thanks to the central limit theorem. This
theorem states that the sum (or average) of a large number of
independent, identically distributed random variables will approach a
normal distribution, regardless of the underlying distribution of the
individual variables. It is difficult to specify what constitutes a
large-enough test set. However, under stronger assumptions than those of
the central limit theorem, we can at least estimate the rate of
convergence to the normal distribution using the BerryâEsseen theorem,
which gives a more quantitative estimate of how quickly the convergence
in the central limit theorem occurs.

</div>
<p>Inamachinelearningcontext,wecantaketheoriginaldatasetanddraw a random
sample <em>with replacement</em>. If the dataset has size <em>n</em> and we draw a
random sample with replacement of size <em>n</em>, this implies that some data points
will likely be duplicated in this new sample, whereas other data points are not sampled at all.We can then repeat this procedure for multiple rounds to obtain multiple training and test sets. This process is known as <em>out-of-bag bootstrapping</em>, illustrated in FigureÂ <a data-reference="fig:ch25-fig04" data-reference-type="ref" href="#fig:ch25-fig04">[fig:ch25-fig04]</a>.</p>
<div class="figurewide">
<img alt="image" src="../images/ch25-fig04.png" style="width:5.625in">
</img></div>
<p>Suppose we constructed <em>k</em> training and test sets. We can now take each
of these splits to train and evaluate the model to obtain <em>k</em> test set
accuracy estimates. Considering this distribution of test set accuracy
estimates, we can take the range between the 2.5th and 97.5th percentile
to obtain the
95 percent confidence interval, as illustrated in
FigureÂ <a data-reference="fig:ch25-fig05" data-reference-type="ref" href="#fig:ch25-fig05">1.2</a>.</p>
<figure id="fig:ch25-fig05">
<img src="../images/ch25-fig05.png">
<figcaption>The distribution of test accuracies from 1,000
bootstrap<br>
samples, including a 95 percent confidence interval</br></figcaption>
</img></figure>
<p>Unlike the normal approximation interval method, we can consider this
out-of-bag bootstrap approach to be more agnostic to the specific
distribution. Ideally, if the assumptions for the normal approximation
are satisfied, both methodologies would yield identical outcomes.</p>
<p>Since bootstrapping relies on resampling the existing test data, its
downside is that it doesnât bring in any new information that could be
available in a broader population or unseen data. Therefore, it may not
always be able to generalize the performance of the model to new, unseen
data.</p>
<p>Note that we are using the bootstrap sampling approach in this chapter
instead of obtaining the train-test splits via <em>k</em>-fold
cross-validation, because of the bootstrapâs theoretical grounding via
the central limit theorem discussed earlier. There are also more
advanced out-of-bag bootstrap methods, such as the .632 and .632+
estimates, which are reweighting the accuracy estimates.</p>
<h3 id="method-3-bootstrapping-test-set-predictions">
        
        
          Method 3: Bootstrapping Test Set Predictions <a href="#method-3-bootstrapping-test-set-predictions"></a>
</h3>
<p>An alternative approach to bootstrapping training sets is to bootstrap test sets.The idea is to train the model on the existing training set as usual and then to evaluate the model on bootstrapped test sets, as illustrated in FigureÂ <a data-reference="fig:ch25-fig06" data-reference-type="ref" href="#fig:ch25-fig06">1.3</a>. After obtaining the test set performance estimates, we can then apply the percentile method described in the previous section.</p>
<figure id="fig:ch25-fig06">
<img src="../images/ch25-fig06.png">
<figcaption>Bootstrapping the test set</figcaption>
</img></figure>
<p>Contrary to the prior bootstrap technique, this method uses a trained
model and simply resamples the test set (instead of the training sets).
This approach is especially appealing for evaluating deep neural
networks, as it doesnât require retraining the model on the new data
splits. However, a disadvantage of this approach is that it doesnât
assess the modelâs variability toward small changes in the training
data.</p>
<h3 id="method-4-retraining-models-with-different-random-seeds">
        
        
          Method 4: Retraining Models with Different Random Seeds <a href="#method-4-retraining-models-with-different-random-seeds"></a>
</h3>
<p>In deep learning, models are commonly retrained using various random
seeds since some random weight initializations may lead to much better
models than others. How can we build a confidence interval from these
experiments? If we assume that the sample means follow a normal
distribution, we can employ a previously discussed method where we
calculate the confidence interval around a sample mean, denoted as
\(\bar{\text{\emph{x}}}\), as follows:</p>
<pre><code class="language-math">\bar{x} \pm z \times \text{SE}
</code></pre>
<p>Since in this context we often work with a relatively modest number of
samples (for instance, models from 5 to 10 random seeds), assuming a <em>t</em>
distribution is deemed more suitable than a normal distribution.
Therefore, we substitute the <em>z</em> value with a <em>t</em> value in the preceding
formula. (As the sample size increases, the <em>t</em> distribution tends to
look more like the standard normal distribution, and the critical values
[<em>z</em> and <em>t</em>] become increasingly similar.)</p>
<p>Furthermore,ifweareinterestedintheaverageaccuracy,denoted
as \(\overline{\text{ACC}}\)<sub>test</sub>, we consider ACC<sub>test,
<em>j</em></sub> corresponding to a unique random seed <em>j</em> as a sample. The
number of random seeds we evaluate would then constitute the sample size
<em>n</em>. As such, we would calculate:</p>
<pre><code class="language-math">\overline{\text{ACC}}_{\text{test}} \pm t \times \text{SE}
</code></pre>
<p>Here, SE is the standard error, calculated as SE =
SD/\(\!\sqrt{\text{\emph{n}}}\), while</p>
<pre><code class="language-math">\overline{\text{ACC}}_{\text{test}} = \frac{1}{r} \sum_{j\,=\,1}^{r} \text{ACC}_{\text{test}, j}
</code></pre>
<p>is the average accuracy, which we compute over the <em>r</em> random seeds. The
standard deviation SD is calculated as follows:</p>
<pre><code class="language-math">\mathrm{SD}=\sqrt{\frac{\sum_j\left(A C C_{\mathrm{test}, j}-\overline{A C C}_{\text {test }}\right)^2}{r-1}}
</code></pre>
<p>Tosummarize,calculatingtheconfidenceintervalsusingvarious
random seeds is another effective alternative. However, it is primarily
beneficial for deep learning models. It proves to be costlier than both
the normal approximation approach (method 1) and bootstrapping the test
set (methodÂ 3), as it necessitates retraining the model. On the bright
side, the outcomes derived from disparate random seeds provide us with a
robust
understanding of the modelâs stability.</p>
<h2 id="recommendations">
        
        
          Recommendations <a href="#recommendations"></a>
</h2>
<p>Each possible method for constructing confidence intervals has its
unique advantages and disadvantages. The normal approximation interval
is cheap to compute but relies on the normality assumption about the
distribution. The out-of-bag bootstrap is agnostic to these assumptions
but is substantially more expensive to compute. A cheaper alternative is
bootstrapping the test only, but this involves bootstrapping a smaller
dataset and may be misleading for small or nonrepresentative test set
sizes. Lastly, constructing confidence intervals from different random
seeds is expensive but can give us additional insights into the modelâs
stability.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>25-1. As mentioned earlier, the most common
choice of confidence level is 95 percent confidence intervals. However,
90 percent and 99 percent are also common. Are 90 percent confidence
intervals smaller or wider than 95 percent confidence intervals, and why
is this the case?</p>
<p>25-2. In ââ on pageÂ , we
created test sets by bootstrapping and then applied the already trained
model to compute the test set accuracy on each of these datasets. Can
you think of a method or modification to obtain these test accuracies
more efficiently?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>A detailed discussion of the pitfalls of concluding statistical
significance from nonoverlapping confidence intervals: Martin
Krzywinski and Naomi Altman, âError Barsâ (2013),
<a href="https://www.nature.com/articles/nmeth.2659">https://www.nature.com/articles/nmeth.2659</a>.</p>
</li>
<li>
<p>A more detailed explanation of the binomial proportion confidence
interval:
<a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval</a>.</p>
</li>
<li>
<p>For a detailed explanation of normal approximation intervals, see
Section 1.7 of my article: âModel Evaluation, Model Selection, and
Algorithm Selection in Machine Learningâ (2018),
<a href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</a>.</p>
</li>
<li>
<p>Additional information on the central limit theorem for inde-
Â pendent and identically distributed random variables:
<a href="https://en.wikipedia.org/wiki/Central_limit_theorem">https://en.wikipedia.org/wiki/Central_limit_theorem</a>.</p>
</li>
<li>
<p>For more on the BerryâEsseen theorem:
<a href="https://en.wikipedia.org/wiki/BerryâEsseen_theorem">https://en.wikipedia.org/wiki/BerryâEsseen_theorem</a>.</p>
</li>
<li>
<p>The .632 bootstrap addresses a pessimistic bias of the regular
out-of-bag bootstrapping approach: Bradley Efron, âEstimating the
Error Rate of a Prediction Rule: Improvement on Cross-Validationâ
(1983), <a href="https://www.jstor.org/stable/2288636">https://www.jstor.org/stable/2288636</a>.</p>
</li>
<li>
<p>The .632+ bootstrap corrects an optimistic bias introduced in the .632
bootstrap: Bradley Efron and Robert Tibshirani, âImprovements on
Cross-Validation: The .632+ Bootstrap Methodâ (1997),
<a href="https://www.jstor.org/stable/2965703">https://www.jstor.org/stable/2965703</a>.</p>
</li>
<li>
<p>A deep learning research paper that discusses bootstrapping the test
set predictions: Benjamin Sanchez-Lengeling et al., âMachine Learning
for Scent: Learning Generalizable Perceptual Representations of Small
Moleculesâ (2019), <a href="https://arxiv.org/abs/1910.10685">https://arxiv.org/abs/1910.10685</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
