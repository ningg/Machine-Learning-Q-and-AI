<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 02
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch02/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 02" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 02</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 02" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch02/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch02/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 02" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch02/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 02" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="chapter-2-self-supervised-learning">
        
        
          Chapter 2: Self-Supervised Learning <a href="#chapter-2-self-supervised-learning"></a>
</h1>
<p><span id="ch02" label="ch02"></span></p>
<p><strong>What is self-supervised learning, when is it useful, and what are the
main approaches to implementing it?</strong></p>
<p><em>Self-supervised learning</em> is a pretraining procedure that lets neural
networks leverage large, unlabeled datasets in a supervised fashion.
This chapter compares self-supervised learning to transfer learning, a
related method for pretraining neural networks, and discusses the
practical applications of self-supervised learning. Finally, it outlines
the main categories of self-supervised learning.</p>
<h2 id="self-supervised-learning-vs-transfer-learning">
        
        
          Self-Supervised Learning vs. Transfer Learning <a href="#self-supervised-learning-vs-transfer-learning"></a>
</h2>
<p>Self-supervised learning is related to transfer learning, a technique in
which a model pretrained on one task is reused as the starting point for
a model on a second task. For example, suppose we are interested in
training an image classifier to classify bird species. In transfer
learning, we would pretrain a convolutional neural network on the
ImageNet dataset, a large, labeled image dataset with many different
categories, including various objects and animals. After pretraining on
the general ImageNet dataset, we would take that pretrained model and
train it on the smaller, more specific target dataset that contains the
bird species of interest. (Often, we just have to change the
class-specific output layer, but we can otherwise adopt the pretrained
network as is.)</p>
<p>FigureÂ <a data-reference="fig:ch02-fig01" data-reference-type="ref" href="#fig:ch02-fig01">1.1</a> illustrates the process of
transfer learning.</p>
<figure id="fig:ch02-fig01">
<img src="../images/ch02-fig01.png">
<figcaption>Pretraining with conventional transfer learning</figcaption>
</img></figure>
<p>Self-supervised learning is an alternative approach to transfer learning
in which the model is pretrained not on labeled data but on <em>unlabeled</em>
data. We consider an unlabeled dataset for which we do not have label
information, and then we find a way to obtain labels from the datasetâs
structure to formulate a prediction task for the neural network, as
illustrated in
FigureÂ <a data-reference="fig:ch02-fig02" data-reference-type="ref" href="#fig:ch02-fig02">1.2</a>. These self-supervised training
tasks are also called <em>pretext tasks</em>.</p>
<figure id="fig:ch02-fig02">
<img src="../images/ch02-fig02.png">
<figcaption>Pretraining with self-supervised learning</figcaption>
</img></figure>
<p>The main difference between transfer learning and self-supervised
learning lies in how we obtain the labels during step 1 in
FiguresÂ <a data-reference="fig:ch02-fig01" data-reference-type="ref" href="#fig:ch02-fig01">1.1</a>
andÂ <a data-reference="fig:ch02-fig02" data-reference-type="ref" href="#fig:ch02-fig02">1.2</a>. In transfer learning, we assume
that the labels are provided alongÂ withÂ theÂ data-
Â set; they are typically created by human labelers. In self-supervised
learning, the labels can be directly derived from the training examples.</p>
<p>A self-supervised learning task could be a missing-word prediction in a
natural language processing context. For example, given the sentence âIt
is beautiful and sunny outside,â we can mask out the word <em>sunny</em>, feed
the network the input âIt is beautiful and [MASK] outside,â and have
the network predict the missing word in the â[MASK]â location.
Similarly, we could remove image patches in a computer vision context
and have the neural network fill in the blanks. These are just two
examples of self-supervised learning tasks; many more methods and
paradigms for this type of learning exist.</p>
<p>In sum, we can think of self-supervised learning on the pretext task as
<em>representation learning</em>. We can take the pretrained model to fine-tune
it on the target task (also known as the <em>downstream</em> task).</p>
<h2 id="leveraging-unlabeled-data">
        
        
          Leveraging Unlabeled Data <a href="#leveraging-unlabeled-data"></a>
</h2>
<p>Large neural network architectures require large amounts of labeled data
to perform and generalize well. However, for many problem areas, we
donât have access to large labeled datasets. With self-supervised
learning, we can leverage unlabeled data. Hence, self-supervised
learning is likely to be useful when working with large neural networks
and with a limited quantity of labeled training data.</p>
<p>Transformer-based architectures that form the basis of LLMs and vision
transformers are known to require self-supervised learning for
pretraining to perform well.</p>
<p>For small neural network models such as multilayer perceptrons with two
or three layers, self-supervised learning is typically considered
neither useful nor necessary.</p>
<p>Self-supervised learning likewise isnât useful in traditional machine learning
with nonparametric models such as tree-based random forests or gradient
boosting. Conventional tree-based methods do not have a fixed parameter
structure (in contrast to the weight matrices, for example). Thus,
conventional tree-based methods are not capable of transfer learning and
are incompatible with self-supervised learning.</p>
<h2 id="self-prediction-and-contrastive-self-supervised-learning">
        
        
          Self-Prediction and Contrastive Self-Supervised Learning <a href="#self-prediction-and-contrastive-self-supervised-learning"></a>
</h2>
<p>There are two main categories of self-supervised learning:
self-prediction and contrastive self-supervised learning. In
<em>self-prediction</em>, illustrated in
FigureÂ <a data-reference="fig:ch02-fig03" data-reference-type="ref" href="#fig:ch02-fig03">1.3</a>, we typically change or hide
parts of the input and train the mo-
Â del to reconstruct the original inputs, such as by using a perturbation
mask that obfuscates certain pixels in an image.</p>
<figure id="fig:ch02-fig03">
<img src="../images/ch02-fig03.png">
<figcaption>Self-prediction after applying a<br>
perturbation mask</br></figcaption>
</img></figure>
<p>A classic example is a denoising autoencoder that learns to remove noise
from an input image. Alternatively, consider a masked autoencoder that
reconstructs the missing parts of an image, as shown in
FigureÂ <a data-reference="fig:ch02-fig04" data-reference-type="ref" href="#fig:ch02-fig04">1.4</a>.</p>
<figure id="fig:ch02-fig04">
<img src="../images/ch02-fig04.png">
<figcaption>A masked autoencoder reconstructing a masked
image</figcaption>
</img></figure>
<p>Missing (masked) input self-prediction methods are also commonly used in
natural language processing contexts. Many generative LLMs, such as GPT,
are trained on a next-word prediction pretext task (GPT will be
discussed at greater length in
ChaptersÂ <a data-reference="ch14" data-reference-type="ref" href="../ch14">[ch14]</a>
andÂ <a data-reference="ch17" data-reference-type="ref" href="../ch17">[ch17]</a>). Here, we feed the network text
fragments, where it has to predict the next word in the sequence (as
weâll discuss further in
ChapterÂ <a data-reference="ch17" data-reference-type="ref" href="../ch17">[ch17]</a>).</p>
<p>In <em>contrastive self-supervised learning</em>, we train the neural network
to learn an embedding space where similar inputs are close to each other
and dissimilar inputs are far apart. In other words, we train the
network to produce embeddings that minimize the distance between similar
training inputs and maximize the distance between dissimilar training
examples.</p>
<p>Letâs discuss contrastive learning using concrete example inputs.
Suppose we have a dataset consisting of random animal images. First, we
draw a random image of a cat (the network does not know the label,
because we assume that the dataset is unlabeled). We then augment,
corrupt, or perturb this cat image, such as by adding a random noise
layer and cropping it differently, as shown in
FigureÂ <a data-reference="fig:ch02-fig05" data-reference-type="ref" href="#fig:ch02-fig05">1.5</a>.</p>
<figure id="fig:ch02-fig05">
<img src="../images/ch02-fig05.png">
<figcaption>Image pairs encountered in contrastive learning</figcaption>
</img></figure>
<p>The perturbed cat image in this figure still shows the same cat, so we
want the network to produce a similar embedding vector. We also consider
a random image drawn from the training set (for example, an elephant,
but again, the network doesnât know the label).</p>
<p>For the cat-elephant pair, we want the network to produce dissimilar
embeddings. This way, we implicitly force the network to capture the
imageâs core content while being somewhat agnostic to small differences
and noise. For example, the simplest form of a contrastive loss is the
<em>L</em><sub>2</sub>-norm (Euclidean distance) between the embeddings
produced by model <em>M</em>(\(\cdot\)). Letâs say we update the model weights
to decrease the distance \(||\)<em>M</em>(cat) â
<em>M</em>(cat\('\))\(||\)<sub>2</sub> and increase the distance
\(||\)<em>M</em>(<em>cat</em>) â <em>M</em>(<em>elephant</em>)\(||\)<sub>2</sub>.</p>
<p>FigureÂ <a data-reference="fig:ch02-fig06" data-reference-type="ref" href="#fig:ch02-fig06">1.6</a> summarizes the central concept
behind contrastive learning for the perturbed image scenario. The model
is shown twice, which is known as a <em>siamese network</em> setup.
Essentially, the same model is utilized in two instances: first, to
generate the embedding for the original training
example, and second, to produce the embedding for the perturbed version
of the sample.</p>
<figure id="fig:ch02-fig06">
<img src="../images/ch02-fig06.png">
<figcaption>Contrastive learning</figcaption>
</img></figure>
<p>This example outlines the main idea behind contrastive learning,
but many subvariants exist. Broadly, we can categorize these into
<em>sample</em> contrastive and <em>dimension</em> contrastive methods. The
elephant-cat example in
FigureÂ <a data-reference="fig:ch02-fig06" data-reference-type="ref" href="#fig:ch02-fig06">1.6</a> illustrates a sample contrastive
method, where we focus on learning embeddings to minimize and maximize
distances between training pairs. In <em>dimension</em>-contrastive approaches,
on the other hand, we focus on making only certain variables in the
embedding representations of similar training pairs appear close to each
other while maximizing the distance of others.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>2-1. How could we apply self-supervised
learning to video data?</p>
<p>2-2. Can
self-supervised learning be used for tabular data represented as rows
and columns? If so, how could we approach this?</p>
<h2 id="references">
        
        
          References <a href="#references"></a>
</h2>
<ul>
<li>
<p>For more on the ImageNet dataset:
<a href="https://en.wikipedia.org/wiki/ImageNet">https://en.wikipedia.org/wiki/ImageNet</a>.</p>
</li>
<li>
<p>An example of a contrastive self-supervised learning method: Ting Chen
et al., âA Simple Framework for Contrastive Learning of
Visual Representationsâ (2020), <a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a>.</p>
</li>
<li>
<p>An example of a dimension-contrastive method: Adrien Bardes, Jean
Ponce, and Yann LeCun, âVICRegL: Self-Supervised Learning of Local
Visual Featuresâ (2022), <a href="https://arxiv.org/abs/2210.01571">https://arxiv.org/abs/2210.01571</a>.</p>
</li>
<li>
<p>If you plan to employ self-supervised learning in practice: Randall
Balestriero et al., âA Cookbook of Self-Supervised Learningâ (2023),
<a href="https://arxiv.org/abs/2304.12210">https://arxiv.org/abs/2304.12210</a>.</p>
</li>
<li>
<p>A paper proposing a method of transfer learning and self-supervised
learning for relatively small multilayer perceptrons on tabular
datasets: Dara Bahri et al., âSCARF: Self-Supervised Contrastive
Learning Using Random Feature Corruptionâ (2021),
<a href="https://arxiv.org/abs/2106.15147">https://arxiv.org/abs/2106.15147</a>.</p>
</li>
<li>
<p>A second paper proposing such a method: Roman Levin et al., âTransfer
Learning with Deep Tabular Modelsâ (2022),
<a href="https://arxiv.org/abs/2206.15306"><em>https://arxiv.org/abs/</em></a>
<a href="https://arxiv.org/abs/2206.15306"><em>2206.15306</em></a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
