<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width initial-scale=1" name="viewport">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="Sebastian Raschka" name="author"/>
<meta content="
      Chapter 14
    " property="og:title"/>
<meta content="
        I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.

      " property="og:description"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch14/" property="og:url">
<meta content="Sebastian Raschka, PhD" property="og:site_name">
<meta content="en_US" property="og:locale">
<meta content="@rasbt" name="twitter:site">
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="twitter:description"/>
<meta content="article" property="og:type"/>
<meta content="" property="article:published_time"/>
<meta content="@rasbt" name="twitter:creator"/>
<meta content="Chapter 14" name="twitter:title"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:image"/>
<title>Chapter 14</title>
<meta content="I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.
" name="description"/>
<link href=" /css/combined_direct_no_sass.css" rel="stylesheet"/>
<link href=" /css/fork-awesome.min.css" rel="stylesheet"/>
<meta content="Chapter 14" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch14/" property="og:url"/>
<meta content="" property="og:image"/>
<meta content="" property="og:description"/>
<meta content="Sebastian Raschka, PhD" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="" property="fb:admins"/>
<meta content="" property="fb:app_id"/>
<link href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch14/" rel="canonical"/>
<link href="/images/favicons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/site.webmanifest" rel="manifest"/>
<link color="#5bbad5" href="/images/favicons/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#ffc40d" name="msapplication-TileColor"/>
<meta content="#ffffff" name="theme-color"/>
</meta></meta></meta></meta></meta></head>
<body>
<img alt="Ahead of AI logo" src="../images/ahead-of-ai-icon.png" style="display: none;"/>
<header class="site-header">
<div class="site-title" style="text-decoration: none; margin-top: 2em;">
<a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
<a href="https://x.com/rasbt"><img alt="Twitter/X icon" height="20" src="../images/twitter-bw.jpg" style="padding-left:20px;"/></a>
<!--<a href="https://threads.net/@sebastianraschka"><img src="/images/logos/threads-logo-alt-small.png" height="20" style="padding-left:5px;" alt="Threads icon"></a>-->
<a href="https://www.linkedin.com/in/sebastianraschka/"><img alt="LinkedIn Icon" height="20" src="../images/linkedin-bw.jpg" style="padding-left:5px;"/></a>
<a href="https://github.com/rasbt"><img alt="GitHub icon" height="20" src="../images/github-bw.jpg" style="padding-left:5px;"/></a>
</div>
<!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
<!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
<!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
<!-- </div>-->
<div class="wrapper">
<nav class="site-nav">
<a class="menu-icon" href="#">
<svg viewbox="0 0 18 15">
<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#424242"></path>
<path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#424242"></path>
<path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#424242"></path>
</svg>
</a>
<div class="trigger">
<!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->
<span style="padding-left:0px;margin-left:0px;"></span>
<a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img alt="Ahead of AI Logo" height="20" src="../images/ahead-of-ai-icon.png"/> Blog</span></a>
<!--<a class="page-link" href="/blog/index.html">Blog</a>-->
<a class="page-link" href="/books">Books</a>
<!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
<!--<a class="page-link" href="/teaching">Courses</a>-->
<a class="page-link" href="https://github.com/rasbt/LLMs-from-scratch">LLMs From Scratch</a>
<!--<a class="page-link" href="/publications">Research</a>-->
<a class="page-link" href="/elsewhere">Talks</a>
<a class="page-link" href="/contact">Contact</a>
<a class="page-link" href="/resources">More</a>
</div>
</nav>
</div>
</header>
<div class="page-content">
<div class="wrapper">
<!-- MathJax script for LaTeX rendering -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Open Graph Metadata -->
<meta content="article" property="og:type"/>
<meta content="Chapter 14" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="https://sebastianraschka.com" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch14/" property="og:url"/>
<meta content="Sebastian Raschka's Blog" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<!-- Twitter Metadata -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Chapter 14" name="twitter:title"/>
<meta content="" name="twitter:description"/>
<meta content="https://sebastianraschka.com" name="twitter:image"/>
<meta content="" name="twitter:image:alt"/>
<div class="post">
<header class="post-header">
<h1 class="post-title" style="text-align: left;">Machine Learning Q and AI</h1>
<h2 class="post-subtitle">30 Essential Questions and Answers on Machine Learning and AI</h2>
<p>
      By Sebastian Raschka. <a href="#table-of-contents">Free to read</a>.
      Published by <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>.<br/>
      Copyright Â© 2024-2025 by Sebastian Raschka.
    </p>
<p>
<img alt="Machine Learning and Q and AI" class="right-image-shadow-30" src="../images/2023-ml-ai-beyond.jpg"/>
</p>
<blockquote>
      Machine learning and AI are moving at a rapid pace. Researchers and practitioners are constantly struggling to keep up with the breadth of concepts and techniques. This book provides bite-sized bits of knowledge for your journey from machine learning beginner to expert, covering topics from various machine learning areas. Even experienced machine learning researchers and practitioners will encounter something new that they can add to their arsenal of techniques.
    </blockquote>
<br/>
<p><strong>ð Print Book:</strong><br/>
<a href="https://amzn.to/4488ahe">Amazon</a><br/>
<a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</p>
<p><strong>ð Read Online:</strong><br/>
<a href="#table-of-contents">Full Book (Free)</a>
</p>
</header>
<article class="post-content">
<!-- Optional: Anchor Headings -->
<h1 id="part-3-natural-language-processing">
        
        
          Part 3: Natural Language Processing <a href="#part-3-natural-language-processing"></a>
</h1>
<h2 id="chapter-14-the-distributional-hypothesis">
        
        
          Chapter 14: The Distributional Hypothesis <a href="#chapter-14-the-distributional-hypothesis"></a>
</h2>
<p><span id="ch14" label="ch14"></span></p>
<p><strong>What is the distributional hypothesis in natural language processing
(NLP)? Where is it used, and how far does it hold true?</strong></p>
<p>The distributional hypothesis is a linguistic theory suggesting that
words occurring in the same contexts tend to have similar meanings,
according to the original source, âDistributional Structureâ by Zellig
S. Harris. Succinctly, the more similar the meanings of two words are,
the more often they appear in similar contexts.</p>
<p>Consider the sentence in
FigureÂ <a data-reference="fig:ch14-fig01" data-reference-type="ref" href="#fig:ch14-fig01">1.1</a>, for example. The words <em>cats</em>
and <em>dogs</em> often occur in similar contexts, and we could replace <em>cats</em>
with <em>dogs</em> without making the sentence sound awkward. We could also
replace <em>cats</em> with <em>hamsters</em>, since both are mammals and pets, and the
sentence would still sound plausible. However, replacing <em>cats</em> with an
unrelated word such as <em>sandwiches</em> would render the sentence clearly
wrong, and replacing <em>cats</em> with the unrelated word <em>driving</em> would also
make the sentence grammatically incorrect.</p>
<figure id="fig:ch14-fig01">
<img src="../images/ch14-fig01.png" style="width:105.0%">
<figcaption>Commonanduncommonwordsinagivencontext</figcaption>
</img></figure>
<p>It is easy to construct counterexamples using polysemous words, that is,
words that have multiple meanings that are related but not identical.
For example, consider the word <em>bank</em>. As a noun, it can refer to a
financial institution, the ârising ground bordering a river,â the âsteep
incline of a hill,â or a âprotective cushioning rimâ (according to the
Merriam-Webster dictionary). It can even be a verb: to bank on something
means to rely or depend on it. These different meanings have different
distributional properties and may not always occur in similar contexts.</p>
<p>Nonetheless, the distributional hypothesis is quite useful. Word em-
Â beddings (introduced in
ChapterÂ <a data-reference="ch01" data-reference-type="ref" href="../ch01">[ch01]</a>) such as Word2vec, as well as many
large language transformer models, rely on this idea. This includes
the
masked language model in BERT and the next-word pretraining task used in
GPT.</p>
<h3 id="word2vec-bert-and-gpt">
        
        
          Word2vec, BERT, and GPT <a href="#word2vec-bert-and-gpt"></a>
</h3>
<p>The Word2vec approach uses a simple,two-layer neuralnetwork to encode words into embedding vectors such that the embedding vectors of similar words are both semantically and syntactically close. There are two ways to train a Word2vec model: the continuous bag-of-words (CBOW) approach and the skip-gram approach.When using CBOW, the Word2vec model learns to predict the current words by using the surrounding context words. Conversely,
in the skip-gram model, Word2vec predicts the
context words from a selected word. While skip-gram is more effective for infrequent words, CBOW is usually faster to train.</p>
<p>After training, word embeddings are placed within the vector space so
that words with common contexts in the corpusâthat is, words with
semantic and syntactic similaritiesâare positioned close to each other,
as illustrated in
FigureÂ <a data-reference="fig:ch14-fig02" data-reference-type="ref" href="#fig:ch14-fig02">1.2</a>. Conversely, dissimilar words
are located farther apart in the embedding space.</p>
<figure id="fig:ch14-fig02">
<img src="../images/ch14-fig02.png" style="width:105.0%">
<figcaption>Word2vec embeddings in a two-dimensional<br>
vector space</br></figcaption>
</img></figure>
<p>BERT is an LLM based on the transformer architecture (see
ChapterÂ <a data-reference="ch08" data-reference-type="ref" href="../ch08">[ch08]</a>) that uses a masked language modeling
approach that involves masking (hiding) some of the words in a sentence.
Its task is to predict these masked words based on the other words in
the sequence, as illustrated in
FigureÂ <a data-reference="fig:ch14-fig03" data-reference-type="ref" href="#fig:ch14-fig03">1.3</a>. This is a form of the
self-supervised learning used to pretrain LLMs (see
ChapterÂ <a data-reference="ch02" data-reference-type="ref" href="../ch02">[ch02]</a> for more on self-supervised learning).
The pretrained model produces embeddings in which similar words (or
tokens) are close in the embedding space.</p>
<figure id="fig:ch14-fig03">
<img src="../images/ch14-fig03.png">
<figcaption>BERTâs pretraining task involves predicting<br>
randomly masked words.</br></figcaption>
</img></figure>
<p>GPT, which like BERT is also an LLM based on the transformer
architecture, functions as a decoder. Decoder-style models like GPT
learn to
predict subsequent words in a sequence based on the preceding ones, as
illustrated in
FigureÂ <a data-reference="fig:ch14-fig04" data-reference-type="ref" href="#fig:ch14-fig04">1.4</a>. GPT contrasts with BERT, an
encoder model,
as it emphasizes predicting what follows rather than encoding the
entire
sequence simultaneously.</p>
<figure id="fig:ch14-fig04">
<img src="../images/ch14-fig04.png">
<figcaption>GPT is pretrained by predicting the<br>
next word.</br></figcaption>
</img></figure>
<p>Where BERT is a bidirectional language model that considers the whole
input sequence, GPT only strictly parses previous sequence elements.
This means BERT is usually better suited for classification tasks,
whereas GPT is more suited for text generation tasks. Similar to BERT,
GPT produces high-quality contextualized word embeddings that capture
semantic similarity.</p>
<h3 id="does-the-hypothesis-hold">
        
        
          Does the Hypothesis Hold? <a href="#does-the-hypothesis-hold"></a>
</h3>
<p>For large datasets, the distributional hypothesis more or less holds
true, making it quite useful for understanding and modeling language
patterns, word relationships, and semantic meanings. For example, this
concept enables techniques like word embedding and semantic analysis,
which, in turn, facilitate natural language processing tasks such as
text classification, sentiment analysis, and machine translation.</p>
<p>In conclusion, while there are counterexamples in which the
distributional hypothesis does not hold, it is a very useful concept
that forms the cornerstone of modern language transformer models.</p>
<h3 id="exercises">
        
        
          Exercises <a href="#exercises"></a>
</h3>
<p>14-1. Does the distributional hypothesis
hold true in the case of homophones, or words that sound the same but
have different meanings, such as <em>there</em> and <em>their</em>?</p>
<p>14-2. Can you think of another domain where a
concept similar to the distributional hypothesis applies? (Hint: think
of other input modalities for neural networks.)</p>
<h3 id="references">
        
        
          References <a href="#references"></a>
</h3>
<ul>
<li>
<p>The original source describing the distributional hypothesis:
Zellig S. Harris, âDistributional Structureâ (1954),
<a href="https://doi.org/10.1080/00437956.1954.11659520"><em>https://doi.org/10.1080/</em></a>
<a href="https://doi.org/10.1080/00437956.1954.11659520"><em>00437956.1954.11659520</em></a>.</p>
</li>
<li>
<p>The paper introducing the Word2vec model: Tomas Mikolov et al.,
âEfficient Estimation of Word Representations in Vector Spaceâ (2013),
<a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.</p>
</li>
<li>
<p>The paper introducing the BERT model: Jacob Devlin et al., âBERT:
Pre-training of Deep Bidirectional Transformers for Language
Understandingâ (2018), <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.</p>
</li>
<li>
<p>The paper introducing the GPT model: Alec Radford and Karthik
Narasimhan, âImproving Language Understanding by Generative
Pre-Trainingâ (2018),
<a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"><em>https://www.semanticscholar.org/paper/Improving</em></a>
<a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"><em>-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0</em></a>
<a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"><em>fe0b668a1cc19f2ec95b5003d0a5035</em></a>.</p>
</li>
<li>
<p>BERT produces embeddings in which similar words (or tokens) are close
in the embedding space: Nelson F. Liu et al., âLinguistic Knowledge
and Transferability of Contextual Representationsâ (2019),
<a href="https://arxiv.org/abs/1903.08855">https://arxiv.org/abs/1903.08855</a>.</p>
</li>
<li>
<p>The paper showing that GPT produces high-quality contextualized
word embeddings that capture semantic similarity: Fabio Petroni et
al., âLanguage Models as Knowledge Bases?â (2019),
<a href="https://arxiv.org/abs/1909.01066">https://arxiv.org/abs/1909.01066</a>.</p>
</li>
</ul>
</article>
<br/>
<hr/>
<div class="book-promotion" style="margin-top: 50px;">
<h2>Support the Author</h2>
<p>You can support the author in the following ways:</p>
<ul>
<li>
        Subscribe to <a href="https://magazine.sebastianraschka.com">Sebastian's Substack blog</a>
</li>
<li>
        Purchase a copy on
        <a href="https://amzn.to/4488ahe">Amazon</a> or
        <a href="https://nostarch.com/machine-learning-q-and-ai">No Starch Press</a>
</li>
<li>
        Write an <a href="https://amzn.to/4488ahe">Amazon review</a>
</li>
</ul>
</div>
<div style="margin-bottom: 50px;"></div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col social-col">
<a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
<a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
<a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
<a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
<a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
<a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&amp;hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
<a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
</div>
<div class="footer-col copyright-col">
<p>© 2013-2025 Sebastian Raschka</p>
</div>
</div>
</div>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>

<script src="/js/anchor.min.js" type="text/javascript"></script>
<script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>
</body>
</html>
