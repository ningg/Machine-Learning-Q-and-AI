# Machine-Learning-Q-and-AI
大模型技术30讲（原版），30 Essential Questions and Answers on Machine Learning and AI

## 1.背景

买了一本《大模型技术30讲》，简单阅读了下，要点突出，对于入门、加深关键点理解，很有用。

但是，也存在问题：《大模型技术30讲》印刷质量真的偏差，而且大部分`术语`，都翻译为中文了，不利于中英对比，特别是 AI 领域基本都是英文的，需要我们熟悉`英文术语`。


因此，准备找到 [原始文档：Machine Learning Q and AI](https://sebastianraschka.com/books/ml-q-and-ai/)，并且，编程将其转存至 github.


## 2.目录

当前工程中，维护的 大模型技术 30 讲，目录如下：

- [Introduction](./pages/_books_ml-q-and-ai-chapters_introduction.md)

### Part I: Neural Networks and Deep Learning

- [Chapter 1: Embeddings, Latent Space, and Representations](./pages/_books_ml-q-and-ai-chapters_ch01.md)
- [Chapter 2: Self-Supervised Learning](./pages/_books_ml-q-and-ai-chapters_ch02.md)
- [Chapter 3: Few-Shot Learning](./pages/_books_ml-q-and-ai-chapters_ch03.md)
- [Chapter 4: The Lottery Ticket  Hypothesis](./pages/_books_ml-q-and-ai-chapters_ch04.md)
- [Chapter 5: Reducing Overfitting with Data](./pages/_books_ml-q-and-ai-chapters_ch05.md)
- [Chapter 6: Reducing Overfitting with Model Modifications](./pages/_books_ml-q-and-ai-chapters_ch06.md)
- [Chapter 7: Multi-GPU Training Paradigms](./pages/_books_ml-q-and-ai-chapters_ch07.md)
- [Chapter 8: The Success of Transformers](./pages/_books_ml-q-and-ai-chapters_ch08.md)
- [Chapter 9: Generative AI Models](./pages/_books_ml-q-and-ai-chapters_ch09.md)
- [Chapter 10: Sources of Randomness](./pages/_books_ml-q-and-ai-chapters_ch10.md)

### Part II: Computer Vision

- [Chapter 11: Calculating the Number of Parameters](./pages/_books_ml-q-and-ai-chapters_ch11.md)
- [Chapter 12: Fully Connected and Convolutional Layers](./pages/_books_ml-q-and-ai-chapters_ch12.md)
- [Chapter 13: Large Training Sets for Vision Transformers](./pages/_books_ml-q-and-ai-chapters_ch13.md)

### Part III: Natural Language Processing

- [Chapter 14: The Distributional Hypothesis](./pages/_books_ml-q-and-ai-chapters_ch14.md)
- [Chapter 15: Data Augmentation for Text](./pages/_books_ml-q-and-ai-chapters_ch15.md)
- [Chapter 16: Self-Attention](./pages/_books_ml-q-and-ai-chapters_ch16.md)
- [Chapter 17: Encoder- and Decoder-Style Transformers](./pages/_books_ml-q-and-ai-chapters_ch17.md)
- [Chapter 18: Using and Fine-Tuning Pretrained Transformers](./pages/_books_ml-q-and-ai-chapters_ch18.md)
- [Chapter 19: Evaluating Generative Large Language Models](./pages/_books_ml-q-and-ai-chapters_ch19.md)

### Part IV: Production and Deployment

- [Chapter 20: Stateless and Stateful Training](./pages/_books_ml-q-and-ai-chapters_ch20.md)
- [Chapter 21: Data-Centric AI](./pages/_books_ml-q-and-ai-chapters_ch21.md)
- [Chapter 22: Speeding Up Inference](./pages/_books_ml-q-and-ai-chapters_ch22.md)
- [Chapter 23: Data Distribution Shifts](./pages/_books_ml-q-and-ai-chapters_ch23.md)

### Part V: Predictive Performance and Model Evaluation

- [Chapter 24: Poisson and Ordinal Regression](./pages/_books_ml-q-and-ai-chapters_ch24.md)
- [Chapter 25: Confidence Intervals](./pages/_books_ml-q-and-ai-chapters_ch25.md)
- [Chapter 26: Confidence Intervals vs. Conformal Predictions](./pages/_books_ml-q-and-ai-chapters_ch26.md)
- [Chapter 27: Proper Metrics](./pages/_books_ml-q-and-ai-chapters_ch27.md)
- [Chapter 28: The k in k-Fold Cross-Validation](./pages/_books_ml-q-and-ai-chapters_ch28.md)
- [Chapter 29: Training and Test Set Discordance](./pages/_books_ml-q-and-ai-chapters_ch29.md)
- [Chapter 30: Limited Labeled Data](./pages/_books_ml-q-and-ai-chapters_ch30.md)



